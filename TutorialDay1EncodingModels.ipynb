{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cognition-And-Vision-Amsterdam-CAVA/UvA2024NeuroAI/blob/main/TutorialDay1EncodingModels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mGXQGStzMtU"
      },
      "source": [
        "# Encoding models tutorial\n",
        "### by Giacomo Aldegheri\n",
        "\n",
        "\\\n",
        "\\\n",
        "In this tutorial, we are going to look at encoding models, also known as *linearizing encoding*, *voxelwise encoding*, etc. The idea behind this approach is to measure the alignment between a model and the brain by fitting a linear regression to predict the brain response from the model's activations for the same images. The synthetic activations predicted from the model are then compared (generally via Pearson's correlation) to the real activations on a held-out test set.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1-5wQe30AqUdLFAzXvc0-v4NOzmMpjN-j' width=800>\n",
        "\n",
        "[This recent paper](https://files.osf.io/v1/resources/t975e/providers/osfstorage/6611e05e219e71316bf6a868?format=pdf&action=download&direct&version=1) provides a nice introduction to the framework.\n",
        "\n",
        "In this tutorial, we will use fMRI data from the [2023 Algonauts challenge](http://algonauts.csail.mit.edu/challenge.html). It is a subset of the [Natural Scenes Dataset (NSD)](https://naturalscenesdataset.org/) only including the visual system. The full dataset includes ~10'000 images per subject, but here, to make computations quicker, we will only use 872 images that were seen by all subjects.\n",
        "\n",
        "This tutorial is heavily indebted to the following resources:\n",
        "\n",
        "- [Algonauts 2023 Challenge tutorial](https://colab.research.google.com/drive/1bLJGP3bAo_hAOwZPHpiSHKlt97X9xsUw?usp=share_link#scrollTo=gjQrI9AlzDqG)\n",
        "- [Voxelwise encoding tutorials by Matteo Visconti di Oleggio Castello (Gallant lab)](https://github.com/gallantlab/voxelwise_tutorials)\n",
        "- [Deep NSD tutorial by Colin Conwell](https://colab.research.google.com/drive/1OalDuiQ6Dwg39XT-BkPMz2XAduPUxQtv?usp=sharing)\n",
        "- [Leyla Tarhan's blog post on variance partitioning](http://lytarhan.rbind.io/post/variancepartitioning/)\n",
        "- [Johan Carlin's blog post on noise ceiling estimation](https://www.johancarlin.com/understanding-noise-ceiling-metrics-rsa-compared-to-spearman-brown.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0VBFOHx36vg"
      },
      "source": [
        "## Setup\n",
        "We start by installing some libraries and importing the ones we will need"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9nm3Hd_Giwx"
      },
      "outputs": [],
      "source": [
        "!pip install nilearn==0.9.2\n",
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tx9aHnLYH-4l"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from nilearn import datasets, plotting\n",
        "import pandas as pd\n",
        "from glob import glob\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
        "from torchvision import transforms\n",
        "from sklearn.decomposition import IncrementalPCA, PCA\n",
        "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV\n",
        "from sklearn.model_selection import check_cv, GridSearchCV\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.base import clone\n",
        "from scipy.stats import sem\n",
        "import random\n",
        "from sentence_transformers import SentenceTransformer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2ZgrC97rliV"
      },
      "outputs": [],
      "source": [
        "# Optional: uncomment if you want to use OpenAI's CLIP model:\n",
        "#!pip install git+https://github.com/openai/CLIP.git\n",
        "#import clip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vg_v8BLTqRcn"
      },
      "source": [
        "## Quick notebook execution\n",
        "It is possible that you will have to restart the notebook and execute it from scratch (e.g. sometimes the Runtime gets disconnected). Toggle this to skip execution of computationally intensive cells (e.g. 3D plots)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OA5gFo46qQwh"
      },
      "outputs": [],
      "source": [
        "quick_execution = False # @param {type:\"boolean\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wIcrA4CLcOl"
      },
      "source": [
        "## Set random seed for reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVXgr0LvLh_3"
      },
      "outputs": [],
      "source": [
        "rand_seed = 123 #@param\n",
        "np.random.seed(rand_seed)\n",
        "random.seed(rand_seed)\n",
        "torch.manual_seed(rand_seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VltnihRoQ3L0"
      },
      "source": [
        "## Get data\n",
        "\n",
        "All the data needed for this tutorial is stored on a public Google Drive folder.\n",
        "\n",
        "You can create a shortcut to this folder in your own Google Drive, so that you don't need to copy the data ot take any space on your own Google Drive.\n",
        "\n",
        "The public folder with the data is [here](https://drive.google.com/drive/folders/1AjDOejWLjfXGkr-hK07SZJ_4ni1nypjw?usp=sharing). You need to select the folder and choose \"Add a shortcut to Drive\".\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=15TNjV__sWCcnBRlxbXNbJfpidx-C6nrk' width=500>\n",
        "\n",
        "Then, we need to 'mount' Google Drive, so that it will be possible to access files stored there (at the path `/content/drive/MyDrive/`) from this Colab notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIWytbUHQ2UF"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "main_dir = '/content/drive/MyDrive/UvA_encodingtutorial' #@param {type:\"string\"}\n",
        "fmri_dir = os.path.join(main_dir, 'fmri_data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-hUQRy_f1yo"
      },
      "outputs": [],
      "source": [
        "# Make sure that the data directory is accessible\n",
        "assert os.path.isdir(main_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2i7GGwxzf6JW"
      },
      "outputs": [],
      "source": [
        "# this should show the directory's content, looking like this:\n",
        "#['/content/drive/MyDrive/UvA_encodingtutorial/fmri_data/subj01',\n",
        "# '/content/drive/MyDrive/UvA_encodingtutorial/fmri_data/subj02',\n",
        "# '/content/drive/MyDrive/UvA_encodingtutorial/fmri_data/subj03',\n",
        "# etc.\n",
        "glob(fmri_dir+'/*')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56IbP7t7fEda"
      },
      "source": [
        "### Set device\n",
        "If you have the possibility, using the GPU (CUDA) runtime will make computations significantly faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPyM-jhffHef"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' #@param ['cpu', 'cuda'] {allow-input: true}\n",
        "if device=='cuda':\n",
        "  assert torch.cuda.is_available()\n",
        "device = torch.device(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDA4lf00ixnJ"
      },
      "source": [
        "# Load and visualize the data\n",
        "\n",
        "Great, now we're all set to start playing around with our data!\n",
        "\n",
        "The fMRI data consists of two ```.npy``` files:\n",
        "- ```lh_training_fmri.npy```: the left hemisphere (LH) fMRI data.\n",
        "- ```rh_training_fmri.npy```: the right hemisphere (RH) fMRI data.\n",
        "\n",
        "Both files are 2D arrays, where each row is a stimulus image and each column is an fMRI vertex.\n",
        "\n",
        "For more information on the fMRI responses please check the [README.txt](https://www.google.com/url?q=https%3A%2F%2Fdrive.google.com%2Ffile%2Fd%2F16oLCaDmUBZuT6z_VGKO-qzwidYDE77Sg%2Fview%3Fusp%3Dshare_link) file from the 2023 Algonauts Challenge.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "N0B1QdJxVA9b"
      },
      "outputs": [],
      "source": [
        "# @title Utilities to plot ROIs and activations\n",
        "\n",
        "def get_roi_mask(roi, hemisphere, subj):\n",
        "\n",
        "  subj_dir = os.path.join(fmri_dir, f'subj{subj:02d}')\n",
        "\n",
        "  # Define the ROI class based on the selected ROI\n",
        "  if roi in [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\"]:\n",
        "      roi_class = 'prf-visualrois'\n",
        "  elif roi in [\"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\"]:\n",
        "      roi_class = 'floc-bodies'\n",
        "  elif roi in [\"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\"]:\n",
        "      roi_class = 'floc-faces'\n",
        "  elif roi in [\"OPA\", \"PPA\", \"RSC\"]:\n",
        "      roi_class = 'floc-places'\n",
        "  elif roi in [\"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\"]:\n",
        "      roi_class = 'floc-words'\n",
        "  elif roi in [\"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"]:\n",
        "      roi_class = 'streams'\n",
        "  elif roi == 'all-vertices':\n",
        "      roi_class = roi\n",
        "\n",
        "  # Load the ROI brain surface maps\n",
        "  fsaverage_roi_class_dir = os.path.join(subj_dir, 'roi_masks',\n",
        "      hemisphere[0]+'h.'+roi_class+'_fsaverage_space.npy')\n",
        "\n",
        "  fsaverage_roi_class = np.load(fsaverage_roi_class_dir)\n",
        "\n",
        "  if roi != 'all-vertices':\n",
        "    challenge_roi_class_dir = os.path.join(subj_dir, 'roi_masks',\n",
        "      hemisphere[0]+'h.'+roi_class+'_challenge_space.npy')\n",
        "\n",
        "    challenge_roi_class = np.load(challenge_roi_class_dir)\n",
        "\n",
        "    roi_map_dir = os.path.join(subj_dir, 'roi_masks',\n",
        "        'mapping_'+roi_class+'.npy')\n",
        "    roi_map = np.load(roi_map_dir, allow_pickle=True).item()\n",
        "\n",
        "    # Select the vertices corresponding to the ROI of interest\n",
        "    roi_mapping = list(roi_map.keys())[list(roi_map.values()).index(roi)]\n",
        "    challenge_roi = np.asarray(challenge_roi_class == roi_mapping, dtype=int)\n",
        "    fsaverage_roi = np.asarray(fsaverage_roi_class == roi_mapping, dtype=int)\n",
        "\n",
        "    return challenge_roi, fsaverage_roi\n",
        "\n",
        "  else:\n",
        "    return None, fsaverage_roi_class\n",
        "\n",
        "\n",
        "def plot_surf_map(subj=1, stat_map=None, hemi='left', roi=None, title=None):\n",
        "  \"\"\"\n",
        "  Plot ROI or statistical\n",
        "  \"\"\"\n",
        "  if roi in [None, 'all-vertices']:\n",
        "    roi = 'all-vertices'\n",
        "\n",
        "  challenge_roi, fsaverage_roi = get_roi_mask(roi, hemi, subj)\n",
        "\n",
        "\n",
        "  # Map the fMRI data onto the brain surface map\n",
        "\n",
        "  if stat_map is None:\n",
        "    fsaverage_response = fsaverage_roi\n",
        "    cmap = 'cool'\n",
        "    colorbar = False\n",
        "  else:\n",
        "    fsaverage_response = np.zeros(len(fsaverage_roi))\n",
        "    if roi != 'all-vertices':\n",
        "      fsaverage_response[np.where(fsaverage_roi)[0][:len(np.where(challenge_roi)[0])]] = \\\n",
        "        stat_map[np.where(challenge_roi)[0]]\n",
        "    else:\n",
        "      fsaverage_response[np.where(fsaverage_roi)[0]] = \\\n",
        "        stat_map\n",
        "    cmap = 'cold_hot'\n",
        "    colorbar = True\n",
        "\n",
        "  if title is None:\n",
        "    title = roi+', '+hemi+' hemisphere'\n",
        "\n",
        "  # Create the interactive brain surface map\n",
        "  fsaverage = datasets.fetch_surf_fsaverage('fsaverage')\n",
        "  view = plotting.view_surf(\n",
        "      surf_mesh=fsaverage['infl_'+hemi],\n",
        "      surf_map=fsaverage_response,\n",
        "      bg_map=fsaverage['sulc_'+hemi],\n",
        "      threshold=1e-14,\n",
        "      cmap=cmap,\n",
        "      colorbar=colorbar,\n",
        "      title=title\n",
        "      )\n",
        "  return view"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eidZIzQBnWKV"
      },
      "outputs": [],
      "source": [
        "# @title Choose subject to visualize:\n",
        "subj = 1 #@param [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"] {type:\"raw\", allow-input: true}\n",
        "subj_dir = os.path.join(fmri_dir, f'subj{subj:02d}')\n",
        "\n",
        "lh_fmri = np.load(os.path.join(subj_dir, 'lh_training_fmri.npy'))\n",
        "rh_fmri = np.load(os.path.join(subj_dir, 'rh_training_fmri.npy'))\n",
        "\n",
        "print('LH fMRI data shape:')\n",
        "print(lh_fmri.shape)\n",
        "print('(Stimulus images × LH vertices)')\n",
        "\n",
        "print('\\nRH fMRI data shape:')\n",
        "print(rh_fmri.shape)\n",
        "print('(Stimulus images × RH vertices)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Lq8CZtXluMl"
      },
      "outputs": [],
      "source": [
        "# @title Visualize all vertices on a brain surface map\n",
        "if not quick_execution:\n",
        "  hemisphere = 'left' #@param ['left', 'right']\n",
        "  view = plot_surf_map(subj=subj, hemi=hemisphere)\n",
        "else:\n",
        "  view = 'Skipping plot (untoggle quick execution to show plot)'\n",
        "view"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxPAxEOEpEE4"
      },
      "source": [
        "## Visualize a chosen ROI on the surface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaau-XOvkPuB"
      },
      "outputs": [],
      "source": [
        "if not quick_execution:\n",
        "  hemisphere = 'left' #@param [\"left\", \"right\"]\n",
        "  roi = \"ventral\" #@param [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\", \"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\", \"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\", \"OPA\", \"PPA\", \"RSC\", \"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\", \"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"]\n",
        "  view = plot_surf_map(subj=subj, hemi=hemisphere, roi=roi)\n",
        "else:\n",
        "  view = 'Skipping plot (untoggle quick execution to show plot)'\n",
        "view"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lIohGWKsMhj"
      },
      "source": [
        "## Stimulus images\n",
        "\n",
        "Now, let's look at the images subjects were seeing in the scanner. All images come from the [COCO dataset](https://cocodataset.org/#home) of natural scenes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-wi2eLysOu4"
      },
      "outputs": [],
      "source": [
        "stim_dir = os.path.join(main_dir, 'stimuli')\n",
        "\n",
        "# Create lists with all training and test image file names, sorted\n",
        "img_list = os.listdir(stim_dir)\n",
        "img_list.sort()\n",
        "print('Total n. of images: ' + str(len(img_list)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-RnGJ03QZbw"
      },
      "source": [
        "## Visualize the fMRI response to selected images\n",
        "\n",
        "Select an image, and see the fMRI response of our participant to that image, across all vertices included in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFt60xxstlLC"
      },
      "outputs": [],
      "source": [
        "if not quick_execution:\n",
        "  img = 0 #@param\n",
        "\n",
        "  #Load the image\n",
        "  img_file = os.path.join(stim_dir, img_list[img])\n",
        "  this_img = Image.open(img_file).convert('RGB')\n",
        "\n",
        "  plt.figure()\n",
        "  plt.axis('off')\n",
        "  plt.imshow(this_img)\n",
        "  plt.title('Image: ' + str(img));\n",
        "\n",
        "  stat_map = lh_fmri[img] if hemisphere == 'left' else rh_fmri[img]\n",
        "\n",
        "  view = plot_surf_map(stat_map=stat_map, subj=subj, hemi=hemisphere)\n",
        "else:\n",
        "  view = 'Skipping plot (untoggle quick execution to show plot)'\n",
        "view"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzKfkDGElHSA"
      },
      "source": [
        "## Visualize the fMRI responses of a chosen ROI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AedeOHxRF1k"
      },
      "outputs": [],
      "source": [
        "if not quick_execution:\n",
        "\n",
        "  img = 10 #@param\n",
        "  hemisphere = 'right' #@param ['left', 'right'] {allow-input: true}\n",
        "  roi = \"early\" #@param [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\", \"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\", \"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\", \"OPA\", \"PPA\", \"RSC\", \"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\", \"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"] {allow-input: true}\n",
        "\n",
        "  # Load the image\n",
        "  img_file = os.path.join(stim_dir, img_list[img])\n",
        "  this_img = Image.open(img_file).convert('RGB')\n",
        "\n",
        "  # Plot the image\n",
        "  plt.figure()\n",
        "  plt.axis('off')\n",
        "  plt.imshow(this_img)\n",
        "  plt.title('Training image: ' + str(img+1));\n",
        "\n",
        "  stat_map = lh_fmri[img] if hemisphere == 'left' else rh_fmri[img]\n",
        "\n",
        "  view = plot_surf_map(stat_map=stat_map, subj=subj, hemi=hemisphere, roi=roi)\n",
        "\n",
        "else:\n",
        "  view = 'Skipping plot (untoggle quick execution to show plot)'\n",
        "\n",
        "view"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V75g4OvzuEx8"
      },
      "source": [
        "# Extract and downsample image features from deep models\n",
        "\n",
        "Deep neural networks are the de facto standard for any computer vision task these days. They can be seen as hierarchical feature extractors, with activation patterns at different layers corresponding to different levels of image descriptions: from edge orientations, to object parts, to full objects.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1-E0TygS1BIYop-a_9mTwNk7xSSOQ_Das' width=600>\n",
        "\n",
        "Figure by [Pantelis Monogioudis](https://pantelis.github.io/cs677/docs/common/lectures/deep-learning-introduction/)\n",
        "\n",
        "\\\n",
        "Here, we extract the activations from one or more specified layers of a **convolutional neural network (CNN)**. For example, we can extract the activations of a *convolutional* layer (which have shape `width x height x number of channels`), and then flatten them into a vector, or extract the activations from a *fully-connected* layer, typically one of the final layers of the network, which are already flattened:\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1zKWFWwOTtkBVwmj_MzJNK9lTmyrwfMVW' width=700>\n",
        "\n",
        "Figure from [here](https://www.analyticsvidhya.com/blog/2022/03/basic-introduction-to-convolutional-neural-network-in-deep-learning/)\n",
        "\n",
        "\\\n",
        "Either way, the intermediate activations of deep neural networks such as [AlexNet](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) are typically very high-dimensional. For example, here by default we are using the activations of the convolutional layer `features.2`, which has 64 * 27 * 27 = 46656 dimensions (height x width x channels of the convolutional activation map).\n",
        "\n",
        "With so many dimensions, and relatively few datapoints (785 training images), it's likely that the network will [*overfit*](https://en.wikipedia.org/wiki/Overfitting) to the training data. In a nutshell, this means that the high number of features will allow the network to perfectly learn the idiosyncrasies of the training set, at the cost of generalization to the test set.\n",
        "\n",
        "To avoid this issue, we use [principal component analysis](https://peterbloem.nl/blog/pca) (PCA), the most common form of dimensionality reduction. We reduce the dimensions to 100, a more reasonable number.\n",
        "\n",
        "\\\n",
        "**NOTE:** this is just a ballpark estimate of the number of dimensions. A more principled approach would be to set a threshold proportion (e.g. 90%) of variance explained in the original data (network activations) and use the number of dimensions needed to reach that (e.g. see [this tutorial](https://www.geo.fu-berlin.de/en/v/soga-py/Advanced-statistics/Multivariate-Approaches/Principal-Component-Analysis/PCA-the-basics/Choose-Principal-Components/index.html#:~:text=A%20widely%20applied%20approach%20is,elbow%20in%20the%20scree%20plot.)). Here, we don't need to worry about that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wJUgkReVHofX"
      },
      "outputs": [],
      "source": [
        "# @title Helper functions to obtain a model and extract its activations\n",
        "\n",
        "def get_vision_model(whichmodel):\n",
        "  \"\"\"\n",
        "  Get computer vision model. This function can be\n",
        "  modified to return a custom model.\n",
        "  \"\"\"\n",
        "  if whichmodel in ['alexnet', 'resnet50']:\n",
        "    model = torch.hub.load('pytorch/vision:v0.10.0', whichmodel)\n",
        "    preprocess = transforms.Compose([\n",
        "      transforms.Resize((224,224)), # resize the images to 224x24 pixels\n",
        "      transforms.ToTensor(), # convert the images to a PyTorch tensor\n",
        "      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # normalize the images color channels\n",
        "    ])\n",
        "  elif whichmodel=='clip':\n",
        "    model, preprocess = clip.load(\"ViT-B/32\")\n",
        "  else:\n",
        "    raise ValueError(f'Model {whichmodel} unknown!')\n",
        "\n",
        "  return model, preprocess\n",
        "\n",
        "\n",
        "def list_layers(model):\n",
        "    \"\"\"\n",
        "    List all layers of the model with their names.\n",
        "    \"\"\"\n",
        "    layers = []\n",
        "    for name, mod in model.named_modules():\n",
        "        layers.append(name)\n",
        "    return layers\n",
        "\n",
        "\n",
        "def register_hooks(model, target_layers):\n",
        "    \"\"\"\n",
        "    Register hooks to extract features from specified layers.\n",
        "    \"\"\"\n",
        "    if not isinstance(target_layers, list):\n",
        "      target_layers = [target_layers]\n",
        "\n",
        "    features = {}\n",
        "\n",
        "    def get_hook(name):\n",
        "        def hook(module, input, output):\n",
        "            features[name] = output.detach()\n",
        "        return hook\n",
        "\n",
        "    for name, layer in model.named_modules():\n",
        "        if name in target_layers:\n",
        "            layer.register_forward_hook(get_hook(name))\n",
        "\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SPfDBYnYho-"
      },
      "outputs": [],
      "source": [
        "class FeatureExtractor():\n",
        "  \"\"\"\n",
        "  Wrapper for the feature extractor.\n",
        "  Loads a specified model and calls it on an input image,\n",
        "  returning the activations for the desired layer(s).\n",
        "  \"\"\"\n",
        "  def __init__(self, modelname, device=device, target_layers=None):\n",
        "    self.modelname = modelname\n",
        "    self.device = device\n",
        "    model, self.preprocess = get_vision_model(modelname)\n",
        "    self.model = model.eval().to(self.device)\n",
        "    if target_layers is not None:\n",
        "      self.create_feature_extractor(target_layers)\n",
        "\n",
        "  def create_feature_extractor(self, target_layers):\n",
        "    self.target_layers = target_layers\n",
        "    self.features = register_hooks(self.model, self.target_layers)\n",
        "\n",
        "  def list_layers(self):\n",
        "    return list_layers(self.model)\n",
        "\n",
        "  def run_model(self, x):\n",
        "\n",
        "    x = x.to(self.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      _ = self.model.encode_image(x) if self.modelname=='clip' else self.model(x)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    self.run_model(x)\n",
        "    features = torch.hstack([torch.flatten(l, start_dim=1) for l in self.features.values()])\n",
        "\n",
        "    return features.detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKHc7jiju95S"
      },
      "source": [
        "## Create the feature extractor for a specified model and layer(s).\n",
        "\n",
        "Here, we use our `FeatureExtractor` class to create a feature extractor that takes images as input, and extracts the activations (features) of the desired layer(s) (`target_layers`) from those images.\n",
        "\n",
        "Please note that `target_layers` can be a single string, but also a list of layers. In that case, the layers' activations are concatenated before applying PCA. To find out what layers are available for a given model, you can do the following:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# Note that we don't pass the 'target_layers'\n",
        "# as a parameter to add them later!\n",
        "feat_extractor = FeatureExtractor(modelname, device=device)\n",
        "\n",
        "feat_extractor.list_layers()\n",
        "```\n",
        " After choosing one or multiple layers from the list, you can do:\n",
        "\n",
        "```\n",
        "feat_extractor.create_feature_extractor(target_layers)\n",
        "```\n",
        "\n",
        "Up to you to experiment and try different layers and models! You can search the literature for ideas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUXZLrSvuUCu"
      },
      "outputs": [],
      "source": [
        "# @title Choose model and layer(s)\n",
        "modelname = 'alexnet' #@param ['alexnet', 'resnet50', 'clip'] {allow-input: true}\n",
        "target_layers = 'features.2' #@param {type: 'string'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbjzEpkN8OxX"
      },
      "outputs": [],
      "source": [
        "# Create the feature extractor\n",
        "feat_extractor = FeatureExtractor(modelname, device=device, target_layers=target_layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewlOsFK7WDQ8"
      },
      "source": [
        "## Use PCA to reduce the dimensionality of the extracted features\n",
        "\n",
        "Here, we define the functions that fit our PCA to the training set, and apply it to the test set. We use the [Incremental PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html) from `scikit-learn`, which is fit to the data in batches rather than all at the same time, to be more memory-efficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PI0SAJh_WHpK"
      },
      "outputs": [],
      "source": [
        "def fit_pca(feature_extractor, dataloader, n_components=100, batch_size=500):\n",
        "\n",
        "  # Define PCA parameters (n. components, batch size)\n",
        "  pca = IncrementalPCA(n_components=n_components, batch_size=batch_size)\n",
        "\n",
        "  # Fit PCA to batch\n",
        "  for d in tqdm(dataloader, total=len(dataloader)):\n",
        "    # Extract features\n",
        "    ft = feature_extractor(d)\n",
        "\n",
        "    # Fit PCA to batch\n",
        "    pca.partial_fit(ft)\n",
        "\n",
        "  return pca"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xP8yMoccEQW6"
      },
      "outputs": [],
      "source": [
        "def extract_features(feature_extractor, dataloader, pca=None):\n",
        "  \"\"\"\n",
        "  Given a FeatureExtractor object, a dataloader,\n",
        "  and optionally a PCA, return the\n",
        "  (dimensionality-reduced) features.\n",
        "  \"\"\"\n",
        "\n",
        "  features = []\n",
        "  for d in tqdm(dataloader, total=len(dataloader)):\n",
        "    # Extract features\n",
        "    ft = feature_extractor(d)\n",
        "\n",
        "    if pca is not None:\n",
        "      # Apply PCA transform (optional)\n",
        "      ft = pca.transform(ft)\n",
        "    features.append(ft)\n",
        "\n",
        "  return np.vstack(features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_-8l2Ywrg6e"
      },
      "source": [
        "## Create image dataset\n",
        "\n",
        "This Pytorch dataset provides images to the model, and applies the relevant preprocessing transformations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5kp9LFGmOmP"
      },
      "outputs": [],
      "source": [
        "class ImageDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Pytorch dataset that loads the images\n",
        "    from our stimulus set.\n",
        "    \"\"\"\n",
        "    def __init__(self, imgs_paths, idxs, transform, device=device):\n",
        "        self.imgs_paths = np.array(imgs_paths)[idxs]\n",
        "        self.transform = transform\n",
        "        self.device = device\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load the image\n",
        "        img_path = self.imgs_paths[idx]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        # Preprocess the image and send it to the chosen device ('cpu' or 'cuda')\n",
        "        if self.transform:\n",
        "            img = self.transform(img).to(self.device)\n",
        "        return img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ylwfQEILsAF"
      },
      "source": [
        "### Split fMRI data into training and test partitions\n",
        "\n",
        "For the moment, we will analyze a single subject. We specify which subject we want to analyze, and load their fMRI data.\n",
        "\n",
        "\n",
        "We assign 90% of our 872 images to the training set, and the remaining 10% to the test set. This will be our train-test partition for the rest of the tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ML97iArbuvKU"
      },
      "outputs": [],
      "source": [
        "# @title Choose subject to analyze:\n",
        "subj = 1 #@param [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"] {type:\"raw\", allow-input: true}\n",
        "subj_dir = os.path.join(fmri_dir, f'subj{subj:02d}')\n",
        "\n",
        "lh_fmri = np.load(os.path.join(subj_dir, 'lh_training_fmri.npy'))\n",
        "rh_fmri = np.load(os.path.join(subj_dir, 'rh_training_fmri.npy'))\n",
        "\n",
        "print('LH fMRI data shape:')\n",
        "print(lh_fmri.shape)\n",
        "print('(Stimulus images × LH vertices)')\n",
        "\n",
        "print('\\nRH fMRI data shape:')\n",
        "print(rh_fmri.shape)\n",
        "print('(Stimulus images × RH vertices)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLU2vQk3FbCI"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Calculate how many stimulus images correspond to 90% of the training data\n",
        "num_train = int(np.round(len(img_list) / 100 * 90))\n",
        "# Shuffle all stimulus images\n",
        "idxs = np.arange(len(img_list))\n",
        "np.random.shuffle(idxs)\n",
        "\n",
        "# Assign 90% of the shuffled stimulus images to the training partition,\n",
        "# and 10% to the test partition\n",
        "idxs_train, idxs_test = idxs[:num_train], idxs[num_train:]\n",
        "\n",
        "print('Training stimulus images: ' + format(len(idxs_train)))\n",
        "print('\\nTest stimulus images: ' + format(len(idxs_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_b9-93KLopA"
      },
      "outputs": [],
      "source": [
        "# Partition the fMRI data for the two hemispheres\n",
        "\n",
        "lh_fmri_train = lh_fmri[idxs_train]\n",
        "lh_fmri_test = lh_fmri[idxs_test]\n",
        "rh_fmri_train = rh_fmri[idxs_train]\n",
        "rh_fmri_test = rh_fmri[idxs_test]\n",
        "\n",
        "# Delete the original data arrays to free memory\n",
        "del lh_fmri, rh_fmri"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-Tq-CtQGu2W"
      },
      "outputs": [],
      "source": [
        "batch_size = 200 #@param\n",
        "# Get the paths of all image files\n",
        "imgs_paths = sorted(list(Path(stim_dir).iterdir()))\n",
        "\n",
        "# The DataLoaders contain the ImageDataset class\n",
        "train_imgs_dataloader = DataLoader(\n",
        "    ImageDataset(imgs_paths, idxs_train, transform=feat_extractor.preprocess),\n",
        "    batch_size=batch_size\n",
        ")\n",
        "test_imgs_dataloader = DataLoader(\n",
        "    ImageDataset(imgs_paths, idxs_test, transform=feat_extractor.preprocess),\n",
        "    batch_size=batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sk-x5Kk9N8HL"
      },
      "outputs": [],
      "source": [
        "# @title Fit PCA with desired number of components\n",
        "\n",
        "n_components = 100 # @param\n",
        "\n",
        "pca = fit_pca(feat_extractor, train_imgs_dataloader,\n",
        "              n_components=n_components, batch_size=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfL_9rzu-BqA"
      },
      "source": [
        "## Linearly map the DNN image features  to fMRI responses\n",
        "\n",
        "Now that we have fit our PCA, we can try using our dimensionality-reduced features to predict the fMRI responses! We will use a simple linear regression for that, and then measure the Pearson correlation between the predicted and real fMRI responses on the test set.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1--rx8qPyuhH37uG53MjAOdS_nGdoK5q0' width=800>\n",
        "\n",
        "Figure from the [2023 Algonauts challenge tutorial](https://colab.research.google.com/drive/1bLJGP3bAo_hAOwZPHpiSHKlt97X9xsUw?usp=share_link)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sn6EECgpKr6L"
      },
      "outputs": [],
      "source": [
        "features_train = extract_features(feat_extractor, train_imgs_dataloader, pca)\n",
        "features_test = extract_features(feat_extractor, test_imgs_dataloader, pca)\n",
        "\n",
        "print('\\nTraining images features:')\n",
        "print(features_train.shape)\n",
        "print('(Training stimulus images × PCA features)')\n",
        "\n",
        "print('\\nValidation images features:')\n",
        "print(features_test.shape)\n",
        "print('(Validation stimulus images × PCA features)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hcm3sylXLL83"
      },
      "outputs": [],
      "source": [
        "# Fit linear regressions on the training data\n",
        "reg_lh = LinearRegression().fit(features_train, lh_fmri_train)\n",
        "reg_rh = LinearRegression().fit(features_train, rh_fmri_train)\n",
        "\n",
        "# Use fitted linear regressions to predict the validation and test fMRI data\n",
        "lh_fmri_val_pred = reg_lh.predict(features_test)\n",
        "rh_fmri_val_pred = reg_rh.predict(features_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4n-RQjtXzGxP"
      },
      "source": [
        "## Evaluate and visualize the encoding model's prediction accuracy (i.e., encoding accuracy) using the test partition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5-uT-S9zIQ0"
      },
      "source": [
        "### Compute the encoding accuracy through a Pearson's correlation\n",
        "\n",
        "Here you will compute your encoding model's prediction accuracy (or encoding accuracy) through a Pearson correlation between the predicted and ground truth fMRI test partition data. The correlation scores indicate how similar the predicted fMRI data is to the ground truth data, namely how accurate your encoding model is in predicting (encoding) fMRI responses to images.\n",
        "\n",
        "Specifically, you will correlate the activity of each predicted fMRI vertex with the activity of the corresponding ground truth fMRI vertex, across the test partition stimulus images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6gMxsL6RUBx"
      },
      "outputs": [],
      "source": [
        "def compute_correlation(X, Y):\n",
        "  \"\"\"\n",
        "  Our main correlation function. Computes the Pearson correlation\n",
        "  between *columns* of two arrays X and Y. In our case, that\n",
        "  corresponds to correlation between a specific vertex's activations\n",
        "  across images.\n",
        "  \"\"\"\n",
        "\n",
        "  X = X - np.mean(X, axis=0)\n",
        "  Y = Y - np.mean(Y, axis=0)\n",
        "\n",
        "  covariance = np.sum(X * Y, axis=0)\n",
        "\n",
        "  std_dev_X = np.sqrt(np.sum(X**2, axis=0))\n",
        "  std_dev_Y = np.sqrt(np.sum(Y**2, axis=0))\n",
        "\n",
        "  correlations = covariance / (std_dev_X * std_dev_Y)\n",
        "\n",
        "  return correlations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcBh78oqzJoa"
      },
      "outputs": [],
      "source": [
        "# Correlation arrays of shape: (# vertices)\n",
        "lh_correlation = compute_correlation(lh_fmri_val_pred, lh_fmri_test)\n",
        "rh_correlation = compute_correlation(rh_fmri_val_pred, rh_fmri_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-CDlIxhzLU0"
      },
      "source": [
        "### Visualize the encoding accuracy of all vertices on a brain surface map\n",
        "\n",
        "We visualize the encoding accuracy of all vertices on a brain surface map. For this we need to map each vertex of the correlation results (in `challenge space`) to the corresponding vertices on the brain surface template (in `fsaverage space`).\n",
        "\n",
        "You need to choose the hemisphere (`'left'` or `'right'`) you want to visualize."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6-xDyS-L23T"
      },
      "outputs": [],
      "source": [
        "if not quick_execution:\n",
        "\n",
        "  hemisphere = 'left' #@param ['left', 'right'] {allow-input: true}\n",
        "\n",
        "  stat_map = lh_correlation if hemisphere == 'left' else rh_correlation\n",
        "\n",
        "  view = plot_surf_map(stat_map=stat_map, subj=subj,\n",
        "                       hemi=hemisphere)\n",
        "else:\n",
        "  view = 'Skipping plot (untoggle quick execution to show plot)'\n",
        "view"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sw5wtsBOLSVV"
      },
      "source": [
        "What can you notice in the resulting map? Which parts of the visual cortex seem to be best predicted?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_SpDiHtNdmV"
      },
      "source": [
        "## Average prediction accuracy across subjects for early and late visual ROIs\n",
        "\n",
        "Now, we move beyond a single subject to our full sample of 8 subjects. We also narrow down our analysis to two specific ROIs: early visual cortex and ventral visual cortex."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A213HRiuO15p"
      },
      "outputs": [],
      "source": [
        "def fit_and_predict(subj, features_train, features_test, train_idxs, test_idxs,\n",
        "                    regression=LinearRegression()):\n",
        "  \"\"\"\n",
        "  Given a subject ID, train and test features, and a regression model,\n",
        "  this function loads the relevant fMRI data, fits the regression on it\n",
        "  and computes the prediction accuracy of the fitted model.\n",
        "  It also returns the fitted regressions - we don't need this now, but we\n",
        "  will later!\n",
        "  \"\"\"\n",
        "\n",
        "  subj_dir = os.path.join(fmri_dir, f'subj{subj:02d}')\n",
        "\n",
        "  lh_fmri = np.load(os.path.join(subj_dir, 'lh_training_fmri.npy'))\n",
        "  rh_fmri = np.load(os.path.join(subj_dir, 'rh_training_fmri.npy'))\n",
        "\n",
        "  lh_fmri_train = lh_fmri[train_idxs]\n",
        "  lh_fmri_test = lh_fmri[test_idxs]\n",
        "  rh_fmri_train = rh_fmri[train_idxs]\n",
        "  rh_fmri_test = rh_fmri[test_idxs]\n",
        "\n",
        "  # Fit linear regressions on the training data\n",
        "  reg_lh = clone(regression)\n",
        "  reg_rh = clone(regression)\n",
        "\n",
        "  reg_lh = reg_lh.fit(features_train, lh_fmri_train)\n",
        "  reg_rh = reg_rh.fit(features_train, rh_fmri_train)\n",
        "\n",
        "  # Use fitted linear regressions to predict the validation fMRI data\n",
        "  lh_fmri_test_pred = reg_lh.predict(features_test)\n",
        "  rh_fmri_test_pred = reg_rh.predict(features_test)\n",
        "\n",
        "  # Correlate predicted and ground-truth values\n",
        "  lh_corrs = compute_correlation(lh_fmri_test_pred, lh_fmri_test)\n",
        "  rh_corrs = compute_correlation(rh_fmri_test_pred, rh_fmri_test)\n",
        "\n",
        "  return lh_corrs, rh_corrs, reg_lh, reg_rh\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkfeHuSeeNw4"
      },
      "outputs": [],
      "source": [
        "def barplot(data, x='roi', y='mean_corr', hue='hemisphere', title=None, noise_ceilings=None):\n",
        "  \"\"\"\n",
        "  Function to show a barplot with an additional\n",
        "  (optional) noise ceiling\n",
        "  \"\"\"\n",
        "\n",
        "  ax = sns.barplot(x=x, hue=hue, y=y, errorbar=('ci', 68), data=data)\n",
        "\n",
        "  if noise_ceilings is not None:\n",
        "    upper_lim = max([nc[1] for nc in noise_ceilings]) + 0.1\n",
        "    ax.set_ylim(0, upper_lim)\n",
        "\n",
        "    bar_width = [b.get_width() for b in ax.patches if b.get_width() != 0][0]\n",
        "\n",
        "    noiseceil_idx = 0\n",
        "\n",
        "    for i, roi in enumerate(prediction_df['roi'].unique()):\n",
        "\n",
        "      nc1 = noise_ceilings[noiseceil_idx]\n",
        "      left1 = i - bar_width\n",
        "      bottom1 = nc1[0]\n",
        "\n",
        "      ax.add_patch(plt.Rectangle((left1, bottom1), bar_width-0.01, nc1[1]-nc1[0], color='grey',\n",
        "                                 alpha=0.3))\n",
        "\n",
        "      nc2 = noise_ceilings[noiseceil_idx+1]\n",
        "      left2 = i\n",
        "      bottom2 = nc2[0]\n",
        "\n",
        "      ax.add_patch(plt.Rectangle((left2, bottom2), bar_width-0.01, nc1[1]-nc1[0], color='grey',\n",
        "                                 alpha=0.3))\n",
        "\n",
        "      noiseceil_idx += 2\n",
        "\n",
        "  plt.title(title)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVFfviOGNc27"
      },
      "outputs": [],
      "source": [
        "roi_1 = 'early' #@param [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\", \"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\", \"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\", \"OPA\", \"PPA\", \"RSC\", \"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\", \"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"] {allow-input: true}\n",
        "roi_2 = 'ventral' #@param [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\", \"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\", \"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\", \"OPA\", \"PPA\", \"RSC\", \"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\", \"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"] {allow-input: true}\n",
        "\n",
        "prediction_df = []\n",
        "\n",
        "for subj in tqdm(range(1, 9)):\n",
        "\n",
        "  lh_corrs, rh_corrs, _, _ = fit_and_predict(subj, features_train, features_test,\n",
        "                                       idxs_train, idxs_test)\n",
        "  roi1_lh = get_roi_mask(roi_1, 'lh', subj)[0][:len(lh_corrs)]\n",
        "  roi2_lh = get_roi_mask(roi_2, 'lh', subj)[0][:len(lh_corrs)]\n",
        "  roi1_rh = get_roi_mask(roi_1, 'rh', subj)[0][:len(rh_corrs)]\n",
        "  roi2_rh = get_roi_mask(roi_2, 'rh', subj)[0][:len(rh_corrs)]\n",
        "\n",
        "  prediction_df.append({\n",
        "      'subject': f'subj-{subj:02d}',\n",
        "      f'mean_corr': np.mean(lh_corrs[np.where(roi1_lh)[0]]),\n",
        "      'hemisphere': 'left',\n",
        "      'roi': roi_1\n",
        "  })\n",
        "  prediction_df.append({\n",
        "      'subject': f'subj-{subj:02d}',\n",
        "      f'mean_corr': np.mean(rh_corrs[np.where(roi1_rh)[0]]),\n",
        "      'hemisphere': 'right',\n",
        "      'roi': roi_1\n",
        "  })\n",
        "  prediction_df.append({\n",
        "      'subject': f'subj-{subj:02d}',\n",
        "      f'mean_corr': np.mean(lh_corrs[np.where(roi2_lh)[0]]),\n",
        "      'hemisphere': 'left',\n",
        "      'roi': roi_2\n",
        "  })\n",
        "  prediction_df.append({\n",
        "      'subject': f'subj-{subj:02d}',\n",
        "      f'mean_corr': np.mean(rh_corrs[np.where(roi2_rh)[0]]),\n",
        "      'hemisphere': 'right',\n",
        "      'roi': roi_2\n",
        "  })\n",
        "\n",
        "prediction_df = pd.DataFrame(prediction_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yfgq5SVYY9td"
      },
      "outputs": [],
      "source": [
        "barplot(prediction_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxF48w4xTRPv"
      },
      "source": [
        "We can see that the prediction is quite good in early visual cortex, less so in later ventral visual cortex."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8FZokyCM1N0"
      },
      "source": [
        "# Across-subject noise ceiling\n",
        "\n",
        "What do the correlations we have found mean? Let's find out what's the *maximum* correlation we can obtain from our data. How? A common approach is described [here](https://www.johancarlin.com/understanding-noise-ceiling-metrics-rsa-compared-to-spearman-brown.html).\n",
        "\n",
        "- We estimate a *lower noise ceiling* by correlating the activations of a held-out subject to the mean activations of the remaining $N-1$ subjects. This is an under-estimate of the true maximum correlation.\n",
        "\n",
        "- And we estimate an *upper noise ceiling* by correlating the activations of each subject to the mean activations of all subjects, *including* that subject. This is an over-estimate of the true maximum correlation.\n",
        "\n",
        "We estimate both of these across all participants, and take the average.\n",
        "The true noise ceiling will lie somewhere in between these two extremes. Because we are fitting parameters (in our linear regression) before estimating the predicted-real correlation, for a fair comparison we need to fit a linear regression here as well (as explained in [this paper](https://www.sciencedirect.com/science/article/pii/S105381192200413X)). We thus try to predict each subject as a linear function of the average across all or all other subjects.\n",
        "\\\n",
        "\\\n",
        "**NOTE:** while this is a standard approach in RSA (as you will see tomorrow), usually in encoding models it's more common to estimate noise ceilings *within* participants, using the variance across presentations of the same stimulus as a measure of noise. See for example the [NSD paper](https://www.nature.com/articles/s41593-021-00962-x) or [this tutorial](https://github.com/gallantlab/voxelwise_tutorials/blob/main/tutorials/notebooks/shortclips/01_plot_explainable_variance.ipynb). Here, for simplicity and consistency with tomorrow's RSA tutorial, we choose the across-subject approach instead.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H68oDBvHfQVu"
      },
      "outputs": [],
      "source": [
        "def compute_noise_ceiling(roi, hemi, regression=LinearRegression()):\n",
        "\n",
        "  # Pre-load the data\n",
        "  all_responses = []\n",
        "\n",
        "  for s in range(1, 9):\n",
        "    subj_dir = os.path.join(fmri_dir, f'subj{s:02d}')\n",
        "\n",
        "    fmri_resp = np.load(os.path.join(subj_dir, f'{hemi}_training_fmri.npy'))\n",
        "\n",
        "    challenge_roi, fsaverage_roi = get_roi_mask(roi, hemi, s)\n",
        "\n",
        "    fsaverage_response = np.zeros((len(fmri_resp), len(fsaverage_roi)))\n",
        "    if s == 1:\n",
        "      common_roi = fsaverage_roi\n",
        "    else:\n",
        "      common_roi *= fsaverage_roi\n",
        "    fsaverage_response[:, np.where(fsaverage_roi)[0]] = \\\n",
        "        fmri_resp[:, np.where(challenge_roi)[0]]\n",
        "\n",
        "    all_responses.append(fsaverage_response)\n",
        "\n",
        "  all_responses = [resp[:, np.where(common_roi)[0]] for resp in all_responses]\n",
        "  all_responses = np.dstack(all_responses)\n",
        "\n",
        "  num_train = int(np.round(len(fmri_resp) / 100 * 90))\n",
        "\n",
        "  all_subjs = np.mean(all_responses, axis=2)\n",
        "\n",
        "  all_correlations_lower = []\n",
        "  all_correlations_upper = []\n",
        "\n",
        "  for s in tqdm(range(0, 8)):\n",
        "    this_subj = all_responses[:, :, s]\n",
        "\n",
        "    other_subjs = np.dstack([all_responses[:, :, :s],\n",
        "                            all_responses[:, :, s+1:]])\n",
        "    other_subjs = np.mean(other_subjs, axis=2)\n",
        "\n",
        "    reg_lower = clone(regression)\n",
        "    reg_upper = clone(regression)\n",
        "\n",
        "    reg_lower = reg_lower.fit(this_subj[:num_train],\n",
        "                              other_subjs[:num_train])\n",
        "    reg_upper = reg_upper.fit(this_subj[:num_train],\n",
        "                              all_subjs[:num_train])\n",
        "\n",
        "    other_subjs_pred = reg_lower.predict(this_subj[num_train:])\n",
        "    all_subjs_pred = reg_upper.predict(this_subj[num_train:])\n",
        "\n",
        "    del reg_lower, reg_upper\n",
        "\n",
        "    corrs_lower = compute_correlation(other_subjs_pred,\n",
        "                                      other_subjs[num_train:])\n",
        "    corrs_upper = compute_correlation(all_subjs_pred,\n",
        "                                      all_subjs[num_train:])\n",
        "\n",
        "    all_correlations_lower.append(np.mean(corrs_lower))\n",
        "    all_correlations_upper.append(np.mean(corrs_upper))\n",
        "\n",
        "    lower = np.mean(all_correlations_lower)\n",
        "    upper = np.mean(all_correlations_upper)\n",
        "\n",
        "  return (lower, upper)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pb2bvCefb6cP"
      },
      "source": [
        "Let's compute the noise ceilings for each of our two ROIs and hemispheres! Please note that this part might be a bit time-consuming to run (~14 minutes with a GPU backend). If you choose the `quick_execution` option, we will simply load precomputed noise ceilings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGPU4nlPMSpn"
      },
      "outputs": [],
      "source": [
        "if not quick_execution:\n",
        "  # Compute the noise ceiling for each ROI and hemisphere\n",
        "  all_noiseceilings = []\n",
        "  for roi in [roi_1, roi_2]:\n",
        "    for hemi in ['lh', 'rh']:\n",
        "      this_nc = compute_noise_ceiling(roi, hemi)\n",
        "      print(roi, hemi, this_nc)\n",
        "      all_noiseceilings.append(this_nc)\n",
        "  all_noiseceilings = np.vstack(all_noiseceilings)\n",
        "else:\n",
        "  assert roi_1=='early' and roi_2=='ventral', 'Precomputed noise ceilings are only for the default ROIs, early and ventral!'\n",
        "  all_noiseceilings = np.loadtxt(os.path.join(main_dir, 'additional_data', 'all_noiseceilings.txt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tgsdoydqix3s"
      },
      "outputs": [],
      "source": [
        "# Plot again, this time with noise ceilings\n",
        "barplot(prediction_df, noise_ceilings=all_noiseceilings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Q5S4pxqINmf"
      },
      "source": [
        "We can see that in early visual cortex, we are not *too* far from the noise ceiling! In ventral visual cortex, however, there's still quite some margin for improvement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-sXUyFAtZwP"
      },
      "source": [
        "# Ridge regression\n",
        "\n",
        "This section is based on [this tutorial](https://github.com/gallantlab/voxelwise_tutorials/blob/main/tutorials/notebooks/shortclips/02_plot_ridge_regression.ipynb) by the Gallant Lab.\n",
        "\n",
        "\\\n",
        "\n",
        "\n",
        "We now take a step back from this complex fMRI data, and go back to \"first principles\".\n",
        "\n",
        "Linear regression is a method to model the relation between some input variables $X \\in \\mathbb{R}^{n \\times p}$ and an output variable $y \\in \\mathbb{R}^n$ (the target). Specifically, linear regression uses a vector of coefficients $w \\in \\mathbb{R}^p$ to predict the output:\n",
        "$$\n",
        "\\hat{y} = Xw\n",
        "$$\n",
        "\n",
        "The \"vanilla\" linear regression we have been using is known as *ordinary least squares* (OLS), and it simply looks for the vector $w$ that minimizes the sum of squared errors between predictions and targets:\n",
        "\n",
        "$$\n",
        "w = \\arg \\min_{w}||Xw-y||^2\n",
        "$$\n",
        "\n",
        "In many cases, though, we want to constrain the parameter vector $w$ by adding a second term to the loss function to be minimize. This second term *regularizes* the regression, preventing over-fitting. Ridge regression is a commonly used example of such regularization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l599u-zF0V4A",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Utility functions\n",
        "\n",
        "COEFS = np.array([0.4, 0.3])\n",
        "\n",
        "def create_regression_toy(n_samples=50, n_features=2, noise=0.3, correlation=0,\n",
        "                          random_state=0):\n",
        "    \"\"\"Create a regression toy dataset.\"\"\"\n",
        "    if n_features > 2:\n",
        "        raise ValueError(\"n_features must be <= 2.\")\n",
        "\n",
        "    # create features\n",
        "    rng = np.random.RandomState(random_state)\n",
        "    X = rng.randn(n_samples, n_features)\n",
        "    X -= X.mean(0)\n",
        "    X /= X.std(0)\n",
        "\n",
        "    # makes correlation(X[:, 1], X[:, 0]) = correlation\n",
        "    if n_features == 2:\n",
        "        X[:, 1] -= (X[:, 0] @ X[:, 1]) * X[:, 0] / (X[:, 0] @ X[:, 0])\n",
        "        X /= X.std(0)\n",
        "        if correlation != 0:\n",
        "            X[:, 1] *= np.sqrt(correlation ** (-2) - 1)\n",
        "            X[:, 1] += X[:, 0] * np.sign(correlation)\n",
        "        X /= X.std(0)\n",
        "\n",
        "    # create linear coefficients\n",
        "    w = COEFS[:n_features]\n",
        "\n",
        "    # create target\n",
        "    y = X @ w\n",
        "    y += rng.randn(*y.shape) * noise\n",
        "\n",
        "    return X, y\n",
        "\n",
        "def l2_loss(X, y, w):\n",
        "    if w.ndim == 1:\n",
        "        w = w[:, None]\n",
        "    return np.sum((X @ w - y[:, None]) ** 2, axis=0)\n",
        "\n",
        "\n",
        "def ridge(X, y, alpha):\n",
        "    n_features = X.shape[1]\n",
        "    return np.linalg.solve(X.T @ X + np.eye(n_features) * alpha, X.T @ y)\n",
        "\n",
        "def plot_1d(X, y, w):\n",
        "    w = np.atleast_1d(w)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(6.7, 2.5))\n",
        "\n",
        "    # left plot: y = f(x)\n",
        "    ax = axes[0]\n",
        "    ax.scatter(X, y, alpha=0.5, color=\"C0\")\n",
        "    ylim = ax.get_ylim()\n",
        "    ax.plot([X.min(), X.max()], [X.min() * w[0], X.max() * w[0]], color=\"C1\")\n",
        "    ax.set(xlabel=\"X[:, 0]\", ylabel=\"y\", ylim=ylim)\n",
        "    ax.grid()\n",
        "    for xx, yy in zip(X[:, 0], y):\n",
        "        ax.plot([xx, xx], [yy, xx * w[0]], c='gray', alpha=0.5)\n",
        "\n",
        "    # right plot: loss = f(w)\n",
        "    ax = axes[1]\n",
        "    w_range = np.linspace(-0.1, 0.8, 100)\n",
        "    ax.plot(w_range, l2_loss(X, y, w_range[None]), color=\"C2\")\n",
        "    ax.scatter([w[0]], l2_loss(X, y, w), color=\"C1\")\n",
        "    ax.set(xlabel=\"w[0]\", ylabel=\"Squared loss\")\n",
        "    ax.grid()\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_2d(X, y, w, flat=True, alpha=None, show_noiseless=True):\n",
        "    from mpl_toolkits import mplot3d  # noqa\n",
        "    w = np.array(w)\n",
        "\n",
        "    fig = plt.figure(figsize=(6.7, 2.5))\n",
        "\n",
        "    #####################\n",
        "    # left plot: y = f(x)\n",
        "\n",
        "    try:  # computed_zorder is only available in matplotlib >= 3.4\n",
        "        ax = fig.add_subplot(121, projection='3d', computed_zorder=False)\n",
        "    except AttributeError:\n",
        "        ax = fig.add_subplot(121, projection='3d')\n",
        "\n",
        "    # to help matplotlib displays scatter points behind any surface, we\n",
        "    # first plot the point below, then the surface, then the points above,\n",
        "    # and use computed_zorder=False.\n",
        "    above = y > X @ w\n",
        "    ax.scatter3D(X[~above, 0], X[~above, 1], y[~above], alpha=0.5, color=\"C0\")\n",
        "\n",
        "    xmin, xmax = X.min(), X.max()\n",
        "    xx, yy = np.meshgrid(np.linspace(xmin, xmax, 10),\n",
        "                         np.linspace(xmin, xmax, 10))\n",
        "    ax.plot_surface(xx, yy, xx * w[0] + yy * w[1], color=[0, 0, 0, 0],\n",
        "                    edgecolor=[1., 0.50, 0.05, 0.50])\n",
        "\n",
        "    # plot the point above *after* the surface\n",
        "    ax.scatter3D(X[above, 0], X[above, 1], y[above], alpha=0.5, color=\"C0\")\n",
        "\n",
        "    ax.set(xlabel=\"X[:, 0]\", ylabel=\"X[:, 1]\", zlabel=\"y\",\n",
        "           zlim=[yy.min(), yy.max()])\n",
        "\n",
        "    #########################\n",
        "    # right plot: loss = f(w)\n",
        "    if flat:\n",
        "        ax = fig.add_subplot(122)\n",
        "\n",
        "        if alpha is not None:\n",
        "          w_ols = np.linalg.solve(X.T @ X, X.T @ y)\n",
        "          wmin = np.min(np.vstack([w_ols, w]), axis=0)\n",
        "          wmax = np.max(np.vstack([w_ols, w]), axis=0)\n",
        "          left_bound = -0.1 if wmin[0] >= -0.1 else wmin[0] - 0.1\n",
        "          right_bound = 0.9 if wmax[0] < 0.9 else wmax[0] + 0.1\n",
        "          down_bound = -0.1 if wmin[1] >= -0.1 else wmin[1] - 0.1\n",
        "          up_bound = 0.9 if wmax[1] < 0.9 else wmax[1] + 0.1\n",
        "        else:\n",
        "          left_bound = -0.1 if w[0] >= -0.1 else w[0] - 0.1\n",
        "          right_bound = 0.9 if w[0] < 0.9 else w[0] + 0.1\n",
        "          down_bound = -0.1 if w[1] >= -0.1 else w[1] - 0.1\n",
        "          up_bound = 0.9 if w[1] < 0.9 else w[1] + 0.1\n",
        "        w0, w1 = np.meshgrid(np.linspace(left_bound, right_bound, 100),\n",
        "                             np.linspace(down_bound, up_bound, 100))\n",
        "        w_range = np.stack([w0.ravel(), w1.ravel()])\n",
        "        zz = l2_loss(X, y, w_range).reshape(w0.shape)\n",
        "        # zz_reg = (w_range ** 2).sum(0).reshape(w0.shape) * alpha\n",
        "        ax.imshow(zz, extent=(w0.min(), w0.max(), w1.min(), w1.max()),\n",
        "                  origin=\"lower\")\n",
        "        im = ax.contourf(w0, w1, zz, levels=20, vmax=zz.max(),\n",
        "                         extent=(w0.min(), w0.max(), w1.min(), w1.max()),\n",
        "                         origin=\"lower\")\n",
        "\n",
        "        ax.scatter([w[0]], [w[1]], color=\"C1\", s=[20], label=\"w\")\n",
        "        if show_noiseless:\n",
        "            ax.scatter([COEFS[0]], [COEFS[1]], color=\"r\", s=[20], marker=\"x\",\n",
        "                       label=\"w_noiseless\")\n",
        "            if alpha is not None:\n",
        "                ax.scatter([w_ols[0]], [w_ols[1]], color=\"C3\", s=[20],\n",
        "                           marker=\"o\", label=\"w_OLS\")\n",
        "            ax.legend(framealpha=0.2)\n",
        "\n",
        "        if alpha is not None:\n",
        "            xlim, ylim = ax.get_xlim(), ax.get_ylim()\n",
        "            angle = np.linspace(-np.pi, np.pi, 100)\n",
        "            radius = np.sqrt(np.sum(w ** 2))\n",
        "            ax.plot(np.cos(angle) * radius, np.sin(angle) * radius, c='k')\n",
        "            ax.set_xlim(xlim), ax.set_ylim(ylim)\n",
        "\n",
        "        ax.set(xlabel=\"w[0]\", ylabel=\"w[1]\")\n",
        "        cbar = plt.colorbar(im, ax=ax)\n",
        "        cbar.ax.set(ylabel=\"Squared loss\")\n",
        "\n",
        "    else:  # 3D version of the right plot\n",
        "        ax = fig.add_subplot(122, projection='3d')\n",
        "        w0, w1 = np.meshgrid(np.linspace(0, 1, 10), np.linspace(0, 1, 10))\n",
        "        w_range = np.stack([w0.ravel(), w1.ravel()])\n",
        "        zz = l2_loss(X, y, w_range).reshape(w0.shape)\n",
        "        ax.plot_surface(w0, w1, zz, color=\"C2\", alpha=0.4, edgecolor='gray')\n",
        "        ax.scatter3D([w[0]], [w[1]], [l2_loss(X, y, w)], color=\"C1\")\n",
        "        ax.set(xlabel=\"w[0]\", ylabel=\"w[1]\", zlabel=\"Squared loss\")\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_kfold2(X, y, alpha=0, fit=True, flip=False):\n",
        "    half = X.shape[0] // 2\n",
        "\n",
        "    if not fit:\n",
        "        fig, ax = plt.subplots(1, 1, figsize=(3.4, 2.5))\n",
        "        ax.scatter(X[:half], y[:half], alpha=0.5, color=\"C0\")\n",
        "        ax.scatter(X[half:], y[half:], alpha=0.5, color=\"C1\")\n",
        "        ax.set(xlabel=\"x1\", ylabel=\"y\")\n",
        "        ax.grid()\n",
        "        fig.tight_layout()\n",
        "        plt.show()\n",
        "        return None\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(6.7, 2.5), sharex=True,\n",
        "                             sharey=True)\n",
        "\n",
        "    w_ridge1 = ridge(X[:half], y[:half], alpha)\n",
        "    w_ridge2 = ridge(X[half:], y[half:], alpha)\n",
        "\n",
        "    ax = axes[0]\n",
        "    if not flip:\n",
        "        ax.scatter(X[:half], y[:half], alpha=0.5, color=\"C0\")\n",
        "    else:\n",
        "        ax.scatter(X[half:], y[half:], alpha=0.5, color=\"C1\")\n",
        "    ax.plot([X.min(), X.max()],\n",
        "            [X.min() * w_ridge1, X.max() * w_ridge1], color=\"C0\")\n",
        "    ax.set(xlabel=\"X[:, 0]\", ylabel=\"y\", title='model 1')\n",
        "    ax.grid()\n",
        "\n",
        "    ax = axes[1]\n",
        "    if flip:\n",
        "        ax.scatter(X[:half], y[:half], alpha=0.5, color=\"C0\")\n",
        "    else:\n",
        "        ax.scatter(X[half:], y[half:], alpha=0.5, color=\"C1\")\n",
        "    ax.plot([X.min(), X.max()],\n",
        "            [X.min() * w_ridge2, X.max() * w_ridge2], color=\"C1\")\n",
        "    ax.set(xlabel=\"X[:, 0]\", ylabel=\"y\", title='model 2')\n",
        "    ax.grid()\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_cv_path(X, y):\n",
        "    losses = []\n",
        "    alphas = np.logspace(-2, 4, 12)\n",
        "\n",
        "    half = X.shape[0] // 2\n",
        "    for alpha in alphas:\n",
        "\n",
        "        w_ridge1 = ridge(X[:half], y[:half], alpha)\n",
        "        w_ridge2 = ridge(X[half:], y[half:], alpha)\n",
        "\n",
        "        losses.append(\n",
        "            l2_loss(X[half:], y[half:], w_ridge1) +\n",
        "            l2_loss(X[:half], y[:half], w_ridge2))\n",
        "\n",
        "    best = np.argmin(losses)\n",
        "\n",
        "    # final cv plot\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(4, 3))\n",
        "    ax.semilogx(alphas, losses, '-o', label=\"candidates\")\n",
        "    ax.set(xlabel=\"alpha\", ylabel=\"cross-validation error\")\n",
        "    ax.plot([alphas[best]], [losses[best]], \"o\", c=\"C3\", label=\"best\")\n",
        "    ax.legend()\n",
        "    fig.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0p3NJ-EVP-E"
      },
      "source": [
        "### Run OLS regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqKO9_9kyMgQ"
      },
      "outputs": [],
      "source": [
        "# Create synthetic data\n",
        "X, y = create_regression_toy(n_features=1)\n",
        "\n",
        "plot_1d(X, y, w=[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohmonEBsyYGp"
      },
      "source": [
        "By varying the linear coefficient w, we can change the prediction accuracy of the model, and thus the squared loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVkBZo3dyUE1"
      },
      "outputs": [],
      "source": [
        "plot_1d(X, y, w=[0.2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYFetSvyygjM"
      },
      "outputs": [],
      "source": [
        "plot_1d(X, y, w=[0.7])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PdKNjbAyn1D"
      },
      "source": [
        "The linear coefficient leading to the minimum squared loss can be found analytically with the formula:\n",
        "\n",
        "$$w = (X^\\top X)^{-1}X^\\top y$$\n",
        "\n",
        "This is the OLS solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rht4Jspyi0a"
      },
      "outputs": [],
      "source": [
        "w_ols = np.linalg.solve(X.T @ X, X.T @ y)\n",
        "\n",
        "plot_1d(X, y, w=w_ols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSoHVKoVzrj4"
      },
      "source": [
        "Linear regression can also be used on more than one feature. On the next toy dataset, we will use two features `X[:, 0]` and `X[:, 1]`. The linear regression model is now a *plane*. Here again, summing the squared errors over all samples gives the squared loss. Plotting the squared loss for every value of `w[0]` and `w[1]`leads to a 2D parabola."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeM9Q4L-zWfU"
      },
      "outputs": [],
      "source": [
        "X, y = create_regression_toy(n_features=2)\n",
        "\n",
        "plot_2d(X, y, w=[0, 0], show_noiseless=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyClgjr65JXn"
      },
      "outputs": [],
      "source": [
        "plot_2d(X, y, w=[0.4, 0], show_noiseless=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CY96t2N5Orw"
      },
      "outputs": [],
      "source": [
        "plot_2d(X, y, w=[0, 0.3], show_noiseless=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Or0lI5UL6V9y"
      },
      "source": [
        "Here again, the OLS solution can be found analytically with the same formula. Note that the OLS solution is not equal to the ground-truth coefficients used to generate the toy dataset (black cross). This is because we added some noise to the target values `y`. We want the solution we find to be as close as possible to the ground-truth coeffficients, as this will allow the regression to generalize to new data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xI9RkIyS5R4S"
      },
      "outputs": [],
      "source": [
        "w_ols = np.linalg.solve(X.T @ X, X.T @ y)\n",
        "plot_2d(X, y, w=w_ols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAfl_7gw76x5"
      },
      "source": [
        "The situation becomes more interesting when the features in `X`are correlated. Here, we add a correlation between the first feature `X[:, 0]`and the second feature `X[:, 1]`. With this correlation, the squared loss function is not isotropic anymore, so the lines of equal loss are now ellipses instead of circles. Thus, when starting from the OLS solution, moving `w` towards the top left leads to a small change in the loss, whereas moving it towards the top right leads to a large change in the loss. This anisotropy makes the OLS solution less robust to noise in some particular directions (deviating more from the ground-truth coefficients).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHo_ISQt5sdX"
      },
      "outputs": [],
      "source": [
        "X, y = create_regression_toy(n_features=2, correlation=0.9)\n",
        "\n",
        "w_ols = np.linalg.solve(X.T @ X, X.T @ y)\n",
        "plot_2d(X, y, w=w_ols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQrhH6bCQAuH"
      },
      "source": [
        "The different robustness to noise can be understood mathematically by the fact that the OLS solution requires inverting the matrix $(X^{\\top}X)$. The matrix inversion amounts to inverting the eigenvalues $\\lambda_k$ of the matrix. When the features are highly correlated, some eigenvalues $lambda_k$ are close to zero, and a small change in the features can have a large effect on the inverse. Thus, having small eigenvalues reduces the stability of the inversion. If the correlation is even higher, the smallest eigenvalues get closer to zero, and the OLS solution becomes even less stable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9nCkb74-8NA"
      },
      "outputs": [],
      "source": [
        "X, y = create_regression_toy(n_features=2, correlation=0.999)\n",
        "\n",
        "w_ols = np.linalg.solve(X.T @ X, X.T @ y)\n",
        "plot_2d(X, y, w=w_ols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcbwkCl9RXIb"
      },
      "outputs": [],
      "source": [
        "X, y = create_regression_toy(n_samples=10, n_features=2, correlation=0.999)\n",
        "\n",
        "w_ols = np.linalg.solve(X.T @ X, X.T @ y)\n",
        "plot_2d(X, y, w=w_ols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVKSAp_XUqQl"
      },
      "source": [
        "When the number of features is larger than the number of samples, the linear system becomes under-determined, which means that the OLS problem has an infinite number of solutions, most of which don't generalize well to new data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OUcF7KbViu7"
      },
      "source": [
        "## Ridge regression\n",
        "\n",
        "To solve the instability and under-determinacy issues of OLS, OLS can be extended to *ridge regression*. Ridge regression considers a different optimization problem:\n",
        "\\\n",
        "\\\n",
        "$$\n",
        "w = \\arg\\min_{w}||Xw - y||^2+\\alpha||w||^2\n",
        "$$\n",
        "\\\n",
        "This optimization problem contains two terms: (i) a *data-fitting term* $||Xw - y||^2$, which ensures the regression correctly fits the training data; and (ii) a regularization term $\\alpha||w||^2$, which forces the coefficients $w$ to be close to zero. The regularization term increases the stability of the solution, at the cost of a bias toward zero.\n",
        "\\\n",
        "\\\n",
        "In the regularization term, `alpha` is a positive hyperparameter that controls the regularization strength. With a smaller `alpha`, the solution will be closer to the OLS solution, and with a larget `alpha`, the solution will be further from the OLS solution and closer to the origin.\n",
        "\\\n",
        "\\\n",
        "To illustrate this effect, the following plot shows the ridge solution for a particular value of `\n",
        "alpha`. The black circle corresponds to the line of equal regularization, whereas the blue ellipses are the lines of equal squared loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZVmvYQ7RibD"
      },
      "outputs": [],
      "source": [
        "X, y = create_regression_toy(n_features=2, correlation=0.9)\n",
        "\n",
        "alpha = 23\n",
        "w_ridge = np.linalg.solve(X.T @ X + np.eye(X.shape[1]) * alpha, X.T @ y)\n",
        "plot_2d(X, y, w_ridge, alpha=alpha)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vruzyetCYw0I"
      },
      "source": [
        "To understand why the regularization term makes the solution more robust to noise, let's consider the ridge solution. The ridge solution can be found analytically with the formula:\n",
        "\\\n",
        "\\\n",
        "$$ w = (X^{\\top}X + \\alpha I)^{-1}X^{\\top}y $$\n",
        "\\\n",
        "where $I$ is the identity matrix. In this formula, we can see that the inverted matrix is now $(X^{\\top}X + \\alpha I)$. Compared to OLS, the additional term $\\alpha I$ adds a positive value `alpha` to all eigenvalues $\\lambda_k$ of $(X^\\top X)$ before the matrix  inversion. Inverting $(\\lambda_k +\\alpha)$ instead of $\\lambda_k$ reduces the instability caused by small eigenvalues. This explains why the ridge solution is more robust to noise than the OLS solution.\n",
        "\\\n",
        "\\\n",
        "In the following plots, we can see that even with a stronger correlation, the ridge solution is still reasonably close to the noiseless ground truth, while the OLS solution would be far off."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05X_ZORzYkNI"
      },
      "outputs": [],
      "source": [
        "X, y = create_regression_toy(n_features=2, correlation=0.999)\n",
        "\n",
        "alpha = 23\n",
        "w_ridge = np.linalg.solve(X.T @ X + np.eye(X.shape[1]) * alpha, X.T @ y)\n",
        "plot_2d(X, y, w_ridge, alpha=alpha)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20XHKxtECHE7"
      },
      "source": [
        "Changing the regularization hyperparameter $\\alpha$ leads to another ridge solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hfb3A3QKQVyQ"
      },
      "outputs": [],
      "source": [
        "X, y = create_regression_toy(n_features=2, correlation=0.999)\n",
        "\n",
        "alpha = 200\n",
        "w_ridge = np.linalg.solve(X.T @ X + np.eye(X.shape[1]) * alpha, X.T @ y)\n",
        "plot_2d(X, y, w_ridge, alpha=alpha)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXyKzpBvCgRQ"
      },
      "source": [
        "## Hyperparameter selection\n",
        "\n",
        "One issue with ridge regression is that the hyperparameter $\\alpha$ is arbitrary. Different choices of hyperparameters lead to different models. To compare these models, we cannot compare the ability to fit the training data, because the best model would just be OLS ($\\alpha = 0$). Instead, we want to compare the ability of each model to generalize to new data. To estimate a model's ability to generalize, we can compute its prediction accuracy on a separate dataset that was not used during the model fitting (i.e. not used to find the coefficients $w$).\n",
        "\n",
        "To illustrate this idea, let's split the dataset into two subsets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqVdNkx_CUkR"
      },
      "outputs": [],
      "source": [
        "X, y = create_regression_toy(n_features=1)\n",
        "\n",
        "plot_kfold2(X, y, fit=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9wOlZUVFALy"
      },
      "source": [
        "Then, we can fit a model on each subset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTiCRCm8D9Uc"
      },
      "outputs": [],
      "source": [
        "alpha = 0.1\n",
        "plot_kfold2(X, y, alpha, fit=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZBiaNUHFNPS"
      },
      "source": [
        "And compute the prediction accuracy of each model on the other subset (its *generalization*)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gKkMQagFHxl"
      },
      "outputs": [],
      "source": [
        "plot_kfold2(X, y, alpha, fit=True, flip=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sc3qYtraFkNy"
      },
      "source": [
        "In this way, we can evaluate the ridge regression (fit with a specific $\\alpha$) on its ability to generalize to new data. If we do that for different hyperparameter candidates $\\alpha$, we can select the model leading to the best out-of-set prediction accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9n53Rxg6FXkT"
      },
      "outputs": [],
      "source": [
        "noise = 0.1\n",
        "X, y = create_regression_toy(n_features=2, noise=noise)\n",
        "plot_cv_path(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8MReOTeF7V9"
      },
      "source": [
        "In the example above, the noise level is low, so the best $\\alpha$ is close to zero, and ridge regression is not much better than OLS. However, if the dataset has more noise, a lower number of samples, or more correlated features, the best hyperparameter can be higher. In this case, ridge regression is better than OLS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QY8VOlJVFiF9"
      },
      "outputs": [],
      "source": [
        "noise = 3\n",
        "X, y = create_regression_toy(n_features=2, noise=noise)\n",
        "plot_cv_path(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLvcNHcaGRBE"
      },
      "source": [
        "When the noise level is too high, the best hyperparameter can be the largest on the grid. It either means that the grid is too small, or that the regression does not find a predictive link between the features and the target. In this case, the model with the lowest generalization error always predicts zero ($w = 0$). Notice the values of the cross-validation error on the y-axis!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGrcuQP7GOM1"
      },
      "outputs": [],
      "source": [
        "noise = 100\n",
        "X, y = create_regression_toy(n_features=2, noise=noise)\n",
        "plot_cv_path(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVIvzqgGG0HU"
      },
      "source": [
        "To summarize, to select the best hyperparameter $\\alpha$, the standard method is to perform a grid search:\n",
        "- Split the training set into two subsets: one subset used to fit the models, and one subset to estimate the prediction accuracy (*validation set*)\n",
        "- Define a number of hyperparameter candidates, for example [0.1, 1, 10, 100].\n",
        "- Fit a separate ridge regression model with each hyperparameter candidate $\\alpha$.\n",
        "- Compute the prediction accuracy on the validation set.\n",
        "- Select the hyperparameter candidate leading to the best validation accuracy.\n",
        "\n",
        "\\\n",
        "To make the grid search less sensitive to the choice of how the training data was split, the process can be repeated for multiple splits. Then, the different prediction accuracies can be averaged over splits before the hyperparameter selection, for a more robust estimate of a given $\\alpha$'s accuracy.\n",
        "Thus, the process is called a *cross-validation*.\n",
        "\\\n",
        "\\\n",
        "Learn more about hyperparameter selection and cross-validation on the [scikit-learn documentation](https://scikit-learn.org/stable/modules/cross_validation.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNzFUNZwLGYF"
      },
      "source": [
        "# Hyperparameter search on our own dataset\n",
        "\n",
        "Ok, enough with toy data! Let's return to our fMRI dataset, and implement hyperparameter search on it. Let's see if any value of $\\alpha$ can help us improve performance on our test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uzfd6kHDW0oB"
      },
      "outputs": [],
      "source": [
        "def generate_leave_one_run_out(n_samples, n_runs, random_seed=None):\n",
        "  \"\"\"\n",
        "  This function is a generator that yields partitions of the data.\n",
        "  Each 'run' (a slice of the data) is used as the validation set in turn.\n",
        "  The rest of the data is used as the training set.\n",
        "  \"\"\"\n",
        "\n",
        "  np.random.seed(random_seed)\n",
        "\n",
        "  run_length = np.ceil(n_samples/n_runs).astype(int)\n",
        "  run_onsets = np.arange(0, n_samples, run_length)\n",
        "\n",
        "  all_val_runs = np.random.permutation(n_runs)\n",
        "\n",
        "  all_samples = np.arange(n_samples)\n",
        "  runs = np.split(all_samples, run_onsets[1:])\n",
        "\n",
        "  for val_run in all_val_runs:\n",
        "    train = np.hstack(\n",
        "        [runs[jj] for jj in range(n_runs) if jj != val_run])\n",
        "    val = np.hstack([runs[jj] for jj in range(n_runs) if jj == val_run])\n",
        "    yield train, val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48m0M7wuc2D1"
      },
      "source": [
        "As cross-validation is computationally heavy, we restrict this analysis to a single subject and hemisphere."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxqNVGuVlk3I"
      },
      "outputs": [],
      "source": [
        "subj = 1 # @param [1, 2, 3, 4, 5, 6, 7, 8]\n",
        "hemisphere = 'left' # @param [\"left\", \"right\"]\n",
        "\n",
        "subj_dir = os.path.join(fmri_dir, f'subj{subj:02d}')\n",
        "\n",
        "fmri_data = np.load(os.path.join(subj_dir,\n",
        "                               hemisphere[0]+'h_training_fmri.npy'))\n",
        "\n",
        "fmri_train = fmri_data[idxs_train]\n",
        "fmri_test = fmri_data[idxs_test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grDnedOMBwo7"
      },
      "outputs": [],
      "source": [
        "# Extract features from our model (no PCA here as it will be done later!)\n",
        "features_train = extract_features(feat_extractor, train_imgs_dataloader, pca=None)\n",
        "features_test = extract_features(feat_extractor, test_imgs_dataloader, pca=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9Yyht5H4rTd"
      },
      "source": [
        "## Define hyperparameter grid search\n",
        "\n",
        "We use [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) from `scikit-learn` to perform our hyperparameter search. Given an estimator (in our case, a `pipeline` comprising PCA and ridge regression) it applies it across folds in a cross-validated way. It then selects the hyperparameter (in our case, $\\alpha$) value with the best performance.\n",
        "\n",
        "Notice that here, we have the PCA as part of the pipeline, instead of using our own function implemented earlier. This is to make it interface more nicely with the `scikit-learn` pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upmFITp1-7Sc"
      },
      "outputs": [],
      "source": [
        "# Define a hyperparameter grid with the values of alpha that we want to try\n",
        "alphas = [0., 0.1, 1.0, 10., 20., 40.]\n",
        "hyperparam_grid={'ridge__alpha': alphas}\n",
        "\n",
        "# Keep n. components fixed, although we could potentially also do a grid search\n",
        "n_components = 100 #@param\n",
        "\n",
        "# Define a pipeline combining PCA and ridge regression\n",
        "pca_ridge_pipe = make_pipeline(\n",
        "    PCA(n_components=n_components),\n",
        "    Ridge()\n",
        ")\n",
        "\n",
        "n_samples = len(idxs_train)\n",
        "n_runs = 5\n",
        "cv = generate_leave_one_run_out(n_samples, 5, random_seed=rand_seed)\n",
        "cv = check_cv(cv)\n",
        "\n",
        "grid_search = GridSearchCV(pca_ridge_pipe, hyperparam_grid, cv=cv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLssqzYaAzPm"
      },
      "outputs": [],
      "source": [
        "grid_search.fit(features_train, fmri_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLryj12ODDSe"
      },
      "outputs": [],
      "source": [
        "mean_mse = -grid_search.cv_results_['mean_test_score']\n",
        "sns.pointplot(x=alphas, y=mean_mse)\n",
        "plt.xlabel('alpha')\n",
        "plt.ylabel('MSE')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRDAUMcVGtFP"
      },
      "source": [
        "We can see that 10 is the best value for $\\alpha$ based on our grid search. If we now call `grid_search.predict()` on the held-out test set, it will directly use the best hyperparameter.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDn_78V9C2CU"
      },
      "outputs": [],
      "source": [
        "fmri_pred = grid_search.predict(features_test)\n",
        "corrs = compute_correlation(fmri_pred, fmri_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0YnAlvSH6yY"
      },
      "outputs": [],
      "source": [
        "# Let's visualize the prediction-target correlations over the brain surface map.\n",
        "view = plot_surf_map(stat_map=corrs, subj=subj,\n",
        "                     hemi=hemisphere)\n",
        "view"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5t27TUBeNCsN"
      },
      "source": [
        "**OPTIONAL EXERCISE:** try integrating the grid search with our code from earlier to perform a grid search for each participant, and see how well the resulting ridge regressions perform on our two ROIs. Also check the best performing alphas: are they the same across participants and hemispheres?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWjbJlQ4fGDF"
      },
      "source": [
        "# Sentence embedding models\n",
        "Our visual system is not a simple object categorization neural network: it represents several semantically complex aspects of real-world scenes, particularly in the higher visual areas. Recently, [a paper](https://arxiv.org/abs/2209.11737) by Adrien Doerig and colleagues has shown that sentence embeddings, high-dimensional representations of scene descriptions extracted by language models, can predict fMRI activity in the visual system extremely well. This is surprising, since these models never received the images as input, only their descriptions, and in turn participants in the scanner never read the descriptions, they only saw the images!\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1-1OKCVBZ22cf1NBN1q1mn_4gsxbkPI5B' width=400>\n",
        "\n",
        "Now, we will run an experiment similar to that of Doerig et al., and use embeddings of scene captions to predict fMRI data from the visual system. As the NSD images are taken from COCO, we have access to those captions!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qV8ZXy5QPv8"
      },
      "source": [
        "Let's start by creating a dataset to load the captions, and an Embedder class to wrap our language models, similar to the FeatureExtractor class we implemented for the vision models earlier. We use the [SentenceTransformers](https://sbert.net/) library to easily get our models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8jLrUWN2LLE"
      },
      "outputs": [],
      "source": [
        "class CaptionDataset(Dataset):\n",
        "    def __init__(self, captions_file, idxs):\n",
        "        self.captions = pd.read_csv(captions_file).iloc[idxs]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.captions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        c1 = self.captions.iloc[idx]['caption_1']\n",
        "        c2 = self.captions.iloc[idx]['caption_2']\n",
        "        c3 = self.captions.iloc[idx]['caption_3']\n",
        "        c4 = self.captions.iloc[idx]['caption_4']\n",
        "        c5 = self.captions.iloc[idx]['caption_5']\n",
        "\n",
        "        return c1, c2, c3, c4, c5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAy7qGbbygH3"
      },
      "outputs": [],
      "source": [
        "class Embedder():\n",
        "  \"\"\"\n",
        "  Simple wrapper class for\n",
        "  sentence embedding models.\n",
        "  \"\"\"\n",
        "  def __init__(self, modelname, device=device):\n",
        "    self.modelname = modelname\n",
        "    self.device = device\n",
        "    model = SentenceTransformer(modelname)\n",
        "    self.model = model.to(self.device)\n",
        "\n",
        "  def __call__(self, x):\n",
        "\n",
        "    embeds = [self.model.encode(list(i)) for i in x]\n",
        "\n",
        "    # Average across 5 captions\n",
        "    embeds = np.mean(np.dstack(embeds), axis=2)\n",
        "\n",
        "    return embeds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BV8AYBeOQb04"
      },
      "source": [
        "## Choose a language model\n",
        "\n",
        "Again, you can read the literature on these models to find out differences between them, and whether anyone has tried fitting them to brain data before!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "SplBbRjAetJ7"
      },
      "outputs": [],
      "source": [
        "lang_model_name = 'all-MiniLM-L6-v2' #@param ['all-mpnet-base-v2', 'all-MiniLM-L6-v2', 'average_word_embeddings_komninos', 'sentence-t5-large'] {allow-input: true}\n",
        "\n",
        "embedder = Embedder(lang_model_name, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-C9e5ywMQ-GA"
      },
      "source": [
        "## Create train and test dataloaders\n",
        "\n",
        "Here, we reuse the train/test partition defined above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZsL0_2CGlMO"
      },
      "outputs": [],
      "source": [
        "captions_path = os.path.join(main_dir, 'additional_data', 'nsdcaptions.csv')\n",
        "\n",
        "# The DataLoaders contain the ImageDataset class\n",
        "train_capts_dataloader = DataLoader(\n",
        "    CaptionDataset(captions_path, idxs_train),\n",
        "    batch_size=batch_size\n",
        ")\n",
        "test_capts_dataloader = DataLoader(\n",
        "    CaptionDataset(captions_path, idxs_test),\n",
        "    batch_size=batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFNoKdm5SGqZ"
      },
      "source": [
        "## Reduce dimensionality using PCA\n",
        "\n",
        "As before, since the embeddings are high-dimensional, we reduce their dimensionality using PCA. Their dimensionality is not *as* high as the DNN features we saw earlier, so we only use 20 components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGs44OmoR5Yw"
      },
      "outputs": [],
      "source": [
        "n_components = 20 # @param\n",
        "\n",
        "pca_lang = fit_pca(embedder, train_capts_dataloader,\n",
        "              n_components=n_components, batch_size=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jFG4PShTyTRb"
      },
      "outputs": [],
      "source": [
        "embeddings_train = extract_features(embedder, train_capts_dataloader, pca=pca_lang)\n",
        "embeddings_test = extract_features(embedder, test_capts_dataloader, pca=pca_lang)\n",
        "\n",
        "print('\\nTraining images features:')\n",
        "print(embeddings_train.shape)\n",
        "print('(Training stimulus images × PCA features)')\n",
        "\n",
        "print('\\nTest images features:')\n",
        "print(embeddings_test.shape)\n",
        "print('(Test stimulus images × PCA features)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCgBBpNFSqWC"
      },
      "source": [
        "Now, we compute the embedding model's performance on each of our two ROIs, across participants."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdZjXfNlxkuv"
      },
      "outputs": [],
      "source": [
        "roi_1 = 'early' #@param [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\", \"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\", \"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\", \"OPA\", \"PPA\", \"RSC\", \"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\", \"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"] {allow-input: true}\n",
        "roi_2 = 'ventral' #@param [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\", \"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\", \"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\", \"OPA\", \"PPA\", \"RSC\", \"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\", \"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"] {allow-input: true}\n",
        "\n",
        "prediction_df = []\n",
        "\n",
        "for subj in tqdm(range(1, 9)):\n",
        "  lh_corrs, rh_corrs, _, _ = fit_and_predict(subj, embeddings_train, embeddings_test,\n",
        "                                       idxs_train, idxs_test)\n",
        "  roi1_lh = get_roi_mask(roi_1, 'lh', subj)[0][:len(lh_corrs)]\n",
        "  roi2_lh = get_roi_mask(roi_2, 'lh', subj)[0][:len(lh_corrs)]\n",
        "  roi1_rh = get_roi_mask(roi_1, 'rh', subj)[0][:len(rh_corrs)]\n",
        "  roi2_rh = get_roi_mask(roi_2, 'rh', subj)[0][:len(rh_corrs)]\n",
        "\n",
        "  prediction_df.append({\n",
        "      'subject': f'subj-{subj:02d}',\n",
        "      f'mean_corr': np.mean(lh_corrs[np.where(roi1_lh)[0]]),\n",
        "      'hemisphere': 'left',\n",
        "      'roi': roi_1\n",
        "  })\n",
        "  prediction_df.append({\n",
        "      'subject': f'subj-{subj:02d}',\n",
        "      f'mean_corr': np.mean(rh_corrs[np.where(roi1_rh)[0]]),\n",
        "      'hemisphere': 'right',\n",
        "      'roi': roi_1\n",
        "  })\n",
        "  prediction_df.append({\n",
        "      'subject': f'subj-{subj:02d}',\n",
        "      f'mean_corr': np.mean(lh_corrs[np.where(roi2_lh)[0]]),\n",
        "      'hemisphere': 'left',\n",
        "      'roi': roi_2\n",
        "  })\n",
        "  prediction_df.append({\n",
        "      'subject': f'subj-{subj:02d}',\n",
        "      f'mean_corr': np.mean(rh_corrs[np.where(roi2_rh)[0]]),\n",
        "      'hemisphere': 'right',\n",
        "      'roi': roi_2\n",
        "  })\n",
        "\n",
        "prediction_df = pd.DataFrame(prediction_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7V_1RWwtJCH"
      },
      "outputs": [],
      "source": [
        "barplot(prediction_df, noise_ceilings=all_noiseceilings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_W9mgkgTAt_"
      },
      "source": [
        "We can notice a pattern opposite to what we saw earlier: now, *ventral* visual cortex is predicted much better than early visual cortex. Given that these representations are less visual and more \"semantic\", this makes sense!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M64C8mQZ050t"
      },
      "source": [
        "# Variance partitioning\n",
        "\n",
        "After having tried these two very different models, a question might have popped in your mind: how do we directly compare them? Do they play unique roles? They might be explaining the same variance in the data: after all, visual features and semantic aspects are highly correlated, e.g. whenever the word \"elephant\" appears, something big and gray will be in the image (see [this paper](https://jov.arvojournals.org/article.aspx?articleid=2785568) for an analysis of these correlations on the NSD).\n",
        "\n",
        "**Variance partitioning** is a simple method that attempts to answer this question. The idea is to take the variance explained by two models together (e.g. the concatenated vision and language features), and subtract from that the variance explained by each single model:\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1-2KwQFk1MNFW1c-CMhrbTXhcTGErJjkM' width=500>\n",
        "\n",
        "Whatever remains is the *unique variance* of the other model. The *shared variance* between models, on the other hand, is the sum of the variance explained by both unique models, minus the variance explained by the joint model. If this sounds confusing, [this blog post](http://lytarhan.rbind.io/post/variancepartitioning/) provides a great introduction to the topic. Once the idea is clear, let's dive into the code!\n",
        "\n",
        "Some examples of the use of variance partitioning in the literature:\n",
        "\n",
        "- [This paper](https://www.frontiersin.org/articles/10.3389/fncom.2015.00135/full) uses it to find that much of the variance in scene-selective visual cortex is shared between low-level visual features (Fourier power) and high-level, 3D aspects of the scene (subjective distance)\n",
        "\n",
        "- [This paper](https://www.sciencedirect.com/science/article/pii/S0896627318309954) uses it to distinguish between low-level and 3D features in predicting different areas of the visual cortex.\n",
        "\n",
        "- [This paper](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009267) uses it to compare the ability of models trained for different tasks (e.g. segmentation, opbject recognition, depth prediction) in explaining visual cortex representations.\n",
        "\n",
        "- [This paper](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006111) uses it to disentangle deep net features and a model based on navigational affordances in explaining activity in scene-selective visual cortex."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxNG0mkAXLQB"
      },
      "source": [
        "## Helper functions to partition the variance\n",
        "\n",
        "We define a helper function to convert correlation coefficients to $R^2$, and another to perform the summations and subtractions illustrated in the figure above. This function takes as input the correlations estimated by all three models (only A, only B, A & B)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cc19-TtsJgw7"
      },
      "outputs": [],
      "source": [
        "def get_rsquareds(corrs):\n",
        "    \"\"\"\n",
        "    Convert correlations to R^2\n",
        "    \"\"\"\n",
        "\n",
        "    return np.square(corrs) * np.sign(corrs)\n",
        "\n",
        "\n",
        "def partition_variance(r_A, r_B, r_AUB,\n",
        "                       compute_rsquared=True):\n",
        "    \"\"\"\n",
        "    Partition variance by subtracting\n",
        "    variance explained by single models\n",
        "    from that explained by both models jointly.\n",
        "\n",
        "    args:\n",
        "    - r_A: correlations (or rsquareds) for model A\n",
        "    - r_B: same for model B\n",
        "    - r_AUB: correlations/rsquareds for concatenated models\n",
        "    - compute_rsquared: set to True when the inputs are correlations,\n",
        "        False if they are already rsquareds\n",
        "    \"\"\"\n",
        "\n",
        "    if compute_rsquared:\n",
        "        r_A = get_rsquareds(r_A)\n",
        "        r_B = get_rsquareds(r_B)\n",
        "        r_AUB = get_rsquareds(r_AUB)\n",
        "\n",
        "    uniquevars_A = r_AUB - r_B\n",
        "    uniquevars_B = r_AUB - r_A\n",
        "    sharedvars = r_A + r_B - r_AUB\n",
        "\n",
        "    return uniquevars_A, uniquevars_B, sharedvars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgETTrKkXp_S"
      },
      "source": [
        "We also define a function that extracts features from two models simultaneously\n",
        "(similar to our `extract_features` function above):\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vT76DNKO4YH0"
      },
      "outputs": [],
      "source": [
        "def extract_mixedfeatures(feat_extr1, feat_extr2,\n",
        "                          dataloader1, dataloader2, pca1=None, pca2=None):\n",
        "  \"\"\"\n",
        "  Extract the features from two models in parallel\n",
        "  \"\"\"\n",
        "\n",
        "  features1 = []\n",
        "  features2 = []\n",
        "  for d1, d2 in tqdm(zip(dataloader1, dataloader2), total=len(dataloader1)):\n",
        "    # Extract features\n",
        "    ft1 = feat_extr1(d1)\n",
        "    ft2 = feat_extr2(d2)\n",
        "\n",
        "    if pca1 is not None:\n",
        "      # Apply PCA transform (optional)\n",
        "      ft1 = pca1.transform(ft1)\n",
        "    if pca2 is not None:\n",
        "      # Apply PCA transform (optional)\n",
        "      ft2 = pca2.transform(ft2)\n",
        "\n",
        "    features1.append(ft1)\n",
        "    features2.append(ft2)\n",
        "\n",
        "  return np.vstack(features1), np.vstack(features2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRrUkMR7LX8b"
      },
      "outputs": [],
      "source": [
        "# @title Choose vision model and layer(s)\n",
        "visionmodel = 'alexnet' #@param ['alexnet', 'resnet50', 'clip'] {allow-input: true}\n",
        "target_layers = 'features.2' #@param {type: 'string'}\n",
        "\n",
        "# Note that target_layers can be a list of strings.\n",
        "# To find out what layers are available, you can do"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LIe9qWmLeXr"
      },
      "outputs": [],
      "source": [
        "# @title Choose language model\n",
        "langmodel = 'all-MiniLM-L6-v2' #@param ['all-mpnet-base-v2', 'all-MiniLM-L6-v2', 'average_word_embeddings_komninos', 'sentence-t5-large'] {allow-input: true}\n",
        "\n",
        "lang_embedder = Embedder(lang_model_name, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7oaiRjdLX8m"
      },
      "outputs": [],
      "source": [
        "# Create image feature extractor\n",
        "feat_extractor_img = FeatureExtractor(visionmodel, device=device, target_layers=target_layers)\n",
        "\n",
        "# Create language embedder\n",
        "feat_extractor_lang = Embedder(langmodel, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAxO6n5m4YHv"
      },
      "outputs": [],
      "source": [
        "n_components_img = 100 # @param\n",
        "\n",
        "img_pca = fit_pca(feat_extractor_img, train_imgs_dataloader,\n",
        "              n_components=n_components_img, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gfehMkZEtoT"
      },
      "outputs": [],
      "source": [
        "n_components_lang = 20 # @param\n",
        "\n",
        "lang_pca = fit_pca(feat_extractor_lang, train_capts_dataloader,\n",
        "                   n_components=n_components_lang, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zt_Q6Fo-M7Xc"
      },
      "outputs": [],
      "source": [
        "img_feats_train, lang_feats_train = extract_mixedfeatures(feat_extractor_img, feat_extractor_lang,\n",
        "                                                          train_imgs_dataloader, train_capts_dataloader,\n",
        "                                                          pca1=img_pca, pca2=lang_pca)\n",
        "img_feats_test, lang_feats_test = extract_mixedfeatures(feat_extractor_img, feat_extractor_lang,\n",
        "                                                      test_imgs_dataloader, test_capts_dataloader,\n",
        "                                                      pca1=img_pca, pca2=lang_pca)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTSv1hjxJMaX"
      },
      "outputs": [],
      "source": [
        "print('\\nTraining images features:')\n",
        "print(img_feats_train.shape)\n",
        "print('\\nTraining language features:')\n",
        "print(lang_feats_train.shape)\n",
        "\n",
        "print('\\nTest images features:')\n",
        "print(img_feats_test.shape)\n",
        "print('\\nTest language features:')\n",
        "print(lang_feats_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdFy_Jt5ZAO9"
      },
      "source": [
        "## Run variance partitioning for each subject and ROI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_BUn7mJ1BH-"
      },
      "outputs": [],
      "source": [
        "roi_1 = 'early' #@param [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\", \"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\", \"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\", \"OPA\", \"PPA\", \"RSC\", \"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\", \"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"] {allow-input: true}\n",
        "roi_2 = 'ventral' #@param [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\", \"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\", \"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\", \"OPA\", \"PPA\", \"RSC\", \"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\", \"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"] {allow-input: true}\n",
        "\n",
        "prediction_df = []\n",
        "\n",
        "for subj in tqdm(range(1, 9)):\n",
        "\n",
        "  lh_corrs_vis, rh_corrs_vis, _, _ = fit_and_predict(subj, img_feats_train, img_feats_test,\n",
        "                                              idxs_train, idxs_test)\n",
        "  lh_corrs_lang, rh_corrs_lang, _, _ = fit_and_predict(subj, lang_feats_train, lang_feats_test,\n",
        "                                                 idxs_train, idxs_test)\n",
        "  lh_corrs_both, rh_corrs_both, _, _ = fit_and_predict(subj, np.hstack((img_feats_train, lang_feats_train)),\n",
        "                                                 np.hstack((img_feats_test, lang_feats_test)),\n",
        "                                                 idxs_train, idxs_test)\n",
        "\n",
        "  lh_unique_vis, lh_unique_lang, lh_shared = partition_variance(lh_corrs_vis, lh_corrs_lang, lh_corrs_both,\n",
        "                                                                compute_rsquared=True)\n",
        "  rh_unique_vis, rh_unique_lang, rh_shared = partition_variance(rh_corrs_vis, rh_corrs_lang, rh_corrs_both,\n",
        "                                                                compute_rsquared=True)\n",
        "\n",
        "  roi1_lh = get_roi_mask(roi_1, 'lh', subj)[0][:len(lh_unique_vis)]\n",
        "  roi2_lh = get_roi_mask(roi_2, 'lh', subj)[0][:len(lh_unique_vis)]\n",
        "  roi1_rh = get_roi_mask(roi_1, 'rh', subj)[0][:len(rh_unique_vis)]\n",
        "  roi2_rh = get_roi_mask(roi_2, 'rh', subj)[0][:len(rh_unique_vis)]\n",
        "\n",
        "  prediction_df.append({\n",
        "      'subject': f'subj-{subj:02d}',\n",
        "      f'variance_explained': np.mean(lh_unique_vis[np.where(roi1_lh)[0]]),\n",
        "      'model': 'unique_vis',\n",
        "      'hemisphere': 'left',\n",
        "      'roi': roi_1\n",
        "  })\n",
        "  prediction_df.append({\n",
        "      'subject': f'subj-{subj:02d}',\n",
        "      f'variance_explained': np.mean(lh_unique_lang[np.where(roi1_lh)[0]]),\n",
        "      'model': 'unique_lang',\n",
        "      'hemisphere': 'left',\n",
        "      'roi': roi_1\n",
        "  })\n",
        "  prediction_df.append({\n",
        "      'subject': f'subj-{subj:02d}',\n",
        "      f'variance_explained': np.mean(lh_shared[np.where(roi1_lh)[0]]),\n",
        "      'model': 'shared',\n",
        "      'hemisphere': 'left',\n",
        "      'roi': roi_1\n",
        "  })\n",
        "\n",
        "  prediction_df.append({\n",
        "      'subject': f'subj-{subj:02d}',\n",
        "      f'variance_explained': np.mean(rh_unique_vis[np.where(roi1_rh)[0]]),\n",
        "      'model': 'unique_vis',\n",
        "      'hemisphere': 'right',\n",
        "      'roi': roi_1\n",
        "  })\n",
        "  prediction_df.append({\n",
        "      'subject': f'subj-{subj:02d}',\n",
        "      f'variance_explained': np.mean(rh_unique_lang[np.where(roi1_rh)[0]]),\n",
        "      'model': 'unique_lang',\n",
        "      'hemisphere': 'right',\n",
        "      'roi': roi_1\n",
        "  })\n",
        "  prediction_df.append({\n",
        "      'subject': f'subj-{subj:02d}',\n",
        "      f'variance_explained': np.mean(rh_shared[np.where(roi1_rh)[0]]),\n",
        "      'model': 'shared',\n",
        "      'hemisphere': 'right',\n",
        "      'roi': roi_1\n",
        "  })\n",
        "\n",
        "  prediction_df.append({\n",
        "      'subject': f'subj-{subj:02d}',\n",
        "      f'variance_explained': np.mean(lh_unique_vis[np.where(roi2_lh)[0]]),\n",
        "      'model': 'unique_vis',\n",
        "      'hemisphere': 'left',\n",
        "      'roi': roi_2\n",
        "  })\n",
        "  prediction_df.append({\n",
        "      'subject': f'subj-{subj:02d}',\n",
        "      f'variance_explained': np.mean(lh_unique_lang[np.where(roi2_lh)[0]]),\n",
        "      'model': 'unique_lang',\n",
        "      'hemisphere': 'left',\n",
        "      'roi': roi_2\n",
        "  })\n",
        "  prediction_df.append({\n",
        "      'subject': f'subj-{subj:02d}',\n",
        "      f'variance_explained': np.mean(lh_shared[np.where(roi2_lh)[0]]),\n",
        "      'model': 'shared',\n",
        "      'hemisphere': 'left',\n",
        "      'roi': roi_2\n",
        "  })\n",
        "\n",
        "  prediction_df.append({\n",
        "      'subject': f'subj-{subj:02d}',\n",
        "      f'variance_explained': np.mean(rh_unique_vis[np.where(roi2_rh)[0]]),\n",
        "      'model': 'unique_vis',\n",
        "      'hemisphere': 'right',\n",
        "      'roi': roi_2\n",
        "  })\n",
        "  prediction_df.append({\n",
        "      'subject': f'subj-{subj:02d}',\n",
        "      f'variance_explained': np.mean(rh_unique_lang[np.where(roi2_rh)[0]]),\n",
        "      'model': 'unique_lang',\n",
        "      'hemisphere': 'right',\n",
        "      'roi': roi_2\n",
        "  })\n",
        "  prediction_df.append({\n",
        "      'subject': f'subj-{subj:02d}',\n",
        "      f'variance_explained': np.mean(rh_shared[np.where(roi2_rh)[0]]),\n",
        "      'model': 'shared',\n",
        "      'hemisphere': 'right',\n",
        "      'roi': roi_2\n",
        "  })\n",
        "\n",
        "\n",
        "\n",
        "prediction_df = pd.DataFrame(prediction_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmBkZdyFtPl9"
      },
      "outputs": [],
      "source": [
        "barplot(prediction_df[prediction_df['hemisphere']=='left'], x='roi', y='variance_explained',\n",
        "        hue='model', title='Left hemisphere')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nKo5ntNrOn_"
      },
      "outputs": [],
      "source": [
        "barplot(prediction_df[prediction_df['hemisphere']=='right'], x='roi', y='variance_explained',\n",
        "        hue='model', title='Right hemisphere')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "643PfBbZHZVs"
      },
      "source": [
        "We can notice several interesting things:\n",
        "- In early visual cortex, most variance is either shared between image features and language features, or it's unique to image features (not too surprising). However, there is a small but non-zero amount of variance explained uniquely by language features!\n",
        "- In ventral visual cortex, conversely, most variance is either unique to language features or shared between vision and language.\n",
        "\n",
        "We also notice something odd: in ventral visual cortex, the amount of unique variance explained by image features is actually **negative**! This means that the variance explained by two models together is less than that explained by one of them alone. This can happen, as Leyla Tarhan explains nicely in her [blog post](http://lytarhan.rbind.io/post/variancepartitioning/), when we use separate train and test partitions, and variance explained is thus a measure of *generalization*. If we have too many redundant features, as can be the case with the two concatenated models, the model can overfit to the training set, meaning that it will perform worse on the test set than a simpler model.\n",
        "\n",
        "Negative unique explained variance is thus a symptom of **overfitting**: can you think of ways, that we have looked at, to reduce overfitting?\n",
        "\\\n",
        "\\\n",
        "**EXERCISE:** try to reduce overfitting using some of the tools we have learned in this tutorial. The negative explained variance should disappear!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJKpD0A_xtMA"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this tutorial, we have shown how brain data can be predicted by qualitatively very different models. One model extracted features from the images participants saw in the scanner, the other from descriptions of those images. As expected, each was best suited to predict a different part of the visual cortex: for the vision model, it was the early regions. For the language model, the later regions, more sensitive to semantics.\n",
        "\n",
        "Of course, this approach is in no way limited to deep neural networks. Any kind of feature, as long as it can be expressed as a vector, can be used to predict brain data using a linear regression.\n",
        "\n",
        "For a very exhaustive exploration of different models' ability to predict visual cortical activity, see [this paper](https://www.biorxiv.org/content/10.1101/2022.03.28.485868v2.abstract) by Colin Conwell et al."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpjzD24oVt8g"
      },
      "source": [
        "# Advanced: Fit banded ridge model\n",
        "\n",
        "This section is based on [this tutorial](https://github.com/gallantlab/voxelwise_tutorials/blob/main/tutorials/notebooks/shortclips/06_plot_banded_ridge_model.ipynb) by the Gallant Lab.\n",
        "\n",
        "\\\n",
        "\n",
        "The banded ridge model (proposed in [this paper](https://www.biorxiv.org/content/10.1101/386318v1.full)) is a generalization of ridge regression that allows for different feature spaces (e.g. the features coming from different models) to have different scaling hyperparameters. How much each model contributes to the explained variance can thus be determined.\n",
        "\n",
        "Just like with standard ridge regression, we determine the optimal hyperparameters using cross-validation. However, we will use slightly more sophisticated tools than for vanilla ridge regression. We will also use the [himalaya](https://github.com/gallantlab/himalaya) package besides scikit-learn.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_CsvIv5Vw7S"
      },
      "outputs": [],
      "source": [
        "!pip install himalaya"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNvEL1QmXG0e"
      },
      "outputs": [],
      "source": [
        "from himalaya.backend import set_backend\n",
        "from himalaya.kernel_ridge import MultipleKernelRidgeCV, Kernelizer, ColumnKernelizer\n",
        "from himalaya.scoring import r2_score_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KZUvu4Qe_x9"
      },
      "outputs": [],
      "source": [
        "backend = set_backend('torch_cuda') if device == 'cuda' else set_backend('numpy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeItNDq6eCcU"
      },
      "source": [
        "Pick a solver to search for the best hyperparameters. Here we just use random search, however feel free to explore other options in the himalaya documentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOqebhs2ROBB"
      },
      "outputs": [],
      "source": [
        "# Here we will use the \"random_search\" solver.\n",
        "solver = \"random_search\"\n",
        "\n",
        "# We can check its specific parameters in the function docstring:\n",
        "solver_function = MultipleKernelRidgeCV.ALL_SOLVERS[solver]\n",
        "print(\"Docstring of the function: %s\" % solver_function.__name__)\n",
        "print(solver_function.__doc__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXc3Iiy3eYuI"
      },
      "source": [
        "As before, we generate our cross validation splits:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwMjlewaerlN"
      },
      "outputs": [],
      "source": [
        "n_samples = len(idxs_train)\n",
        "cv = generate_leave_one_run_out(n_samples, 5, random_seed=rand_seed)\n",
        "cv = check_cv(cv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_8gxuXcRPPu"
      },
      "outputs": [],
      "source": [
        "n_iter = 20\n",
        "\n",
        "alphas = np.logspace(1, 20, 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciiVTL-YyOin"
      },
      "outputs": [],
      "source": [
        "n_targets_batch = 100\n",
        "n_alphas_batch = 5\n",
        "n_targets_batch_refit = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nnDoFIIyVub"
      },
      "outputs": [],
      "source": [
        "solver_params = dict(n_iter=n_iter, alphas=alphas,\n",
        "                     n_targets_batch=n_targets_batch,\n",
        "                     n_alphas_batch=n_alphas_batch,\n",
        "                     n_targets_batch_refit=n_targets_batch_refit)\n",
        "\n",
        "mkr_model = MultipleKernelRidgeCV(kernels=\"precomputed\", solver=solver,\n",
        "                                  solver_params=solver_params, cv=cv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvCoytXosa1l"
      },
      "outputs": [],
      "source": [
        "# @title Choose vision model and layer(s)\n",
        "visionmodel = 'alexnet' #@param ['alexnet', 'resnet50', 'clip'] {allow-input: true}\n",
        "target_layers = 'features.2' #@param {type: 'string'}\n",
        "\n",
        "# Note that target_layers can be a list of strings.\n",
        "# To find out what layers are available, you can do"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKYwu5N7sa1q"
      },
      "outputs": [],
      "source": [
        "# @title Choose language model\n",
        "langmodel = 'all-MiniLM-L6-v2' #@param ['all-mpnet-base-v2', 'all-MiniLM-L6-v2', 'average_word_embeddings_komninos', 'sentence-t5-large'] {allow-input: true}\n",
        "\n",
        "lang_embedder = Embedder(lang_model_name, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycOnpAK5sa1q"
      },
      "outputs": [],
      "source": [
        "# Create image feature extractor\n",
        "feat_extractor_img = FeatureExtractor(visionmodel, device=device, target_layers=target_layers)\n",
        "\n",
        "# Create language embedder\n",
        "feat_extractor_lang = Embedder(langmodel, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSMKTlgXsa1q"
      },
      "outputs": [],
      "source": [
        "n_components_img = 100 # @param\n",
        "\n",
        "img_pca = fit_pca(feat_extractor_img, train_imgs_dataloader,\n",
        "              n_components=n_components_img, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9uEGJpssa1q"
      },
      "outputs": [],
      "source": [
        "n_components_lang = 20 # @param\n",
        "\n",
        "lang_pca = fit_pca(feat_extractor_lang, train_capts_dataloader,\n",
        "                   n_components=n_components_lang, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KpmBEQoAsa1q"
      },
      "outputs": [],
      "source": [
        "img_feats_train, lang_feats_train = extract_mixedfeatures(feat_extractor_img, feat_extractor_lang,\n",
        "                                                          train_imgs_dataloader, train_capts_dataloader,\n",
        "                                                          pca1=img_pca, pca2=lang_pca)\n",
        "img_feats_test, lang_feats_test = extract_mixedfeatures(feat_extractor_img, feat_extractor_lang,\n",
        "                                                      test_imgs_dataloader, test_capts_dataloader,\n",
        "                                                      pca1=img_pca, pca2=lang_pca)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isqLkBLRsa1q"
      },
      "outputs": [],
      "source": [
        "print('\\nTraining images features:')\n",
        "print(img_feats_train.shape)\n",
        "print('\\nTraining language features:')\n",
        "print(lang_feats_train.shape)\n",
        "\n",
        "print('\\nTest images features:')\n",
        "print(img_feats_test.shape)\n",
        "print('\\nTest language features:')\n",
        "print(lang_feats_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQToKxpps2fS"
      },
      "source": [
        "Now we define our banded ridge regression, combining the features of the two models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8htheD_kvcys"
      },
      "outputs": [],
      "source": [
        "model_names = ['vision', 'language']\n",
        "slices = [slice(0, n_components_img), slice(n_components_img+1, n_components_img+n_components_lang)]\n",
        "kernelizer = Kernelizer(kernel='linear')\n",
        "kernelizers_tuples = [(name, kernelizer, slice_) for name, slice_ in zip(model_names, slices)]\n",
        "column_kernelizer = ColumnKernelizer(kernelizers_tuples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LG1rwrZpxwMf"
      },
      "outputs": [],
      "source": [
        "# Visualize the kernelizer\n",
        "column_kernelizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQoCD5mLyDmo"
      },
      "outputs": [],
      "source": [
        "pipeline = make_pipeline(\n",
        "    column_kernelizer,\n",
        "    mkr_model,\n",
        ")\n",
        "pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bC6dn0wRbH8Q"
      },
      "source": [
        "## Pick subject\n",
        "As this is also a computationally intense pipeline, we run a single subject."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxlEom0Z7QjG"
      },
      "outputs": [],
      "source": [
        "subj = 1 #@param [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"] {type:\"raw\", allow-input: true}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwZjTNWLy-_D"
      },
      "outputs": [],
      "source": [
        "lh_corrs_both, rh_corrs_both, pipe_lh, pipe_rh = fit_and_predict(subj, np.hstack((img_feats_train, lang_feats_train)),\n",
        "                                                 np.hstack((img_feats_test, lang_feats_test)),\n",
        "                                                 idxs_train, idxs_test, regression=pipeline)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3WWQ5NmCf6z"
      },
      "source": [
        "## Plot the banded ridge split\n",
        "\n",
        "Besides improving prediction accuracy, banded ridge regression provides a way to disentangle the contribution of the two feature spaces. To do so, we take the kernel weights and the ridge (dual) weights corresponding to each feature space, and use them to compute the prediction from each feature space separately.\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\sum_i^{m}\\hat{y}_i = \\sum_i^{m}\\gamma_iK_i\\hat{w}\n",
        "$$\n",
        "\n",
        "Then, we use these split predictions to compute split $\\tilde{R}_i^2$ scores. These scores are corrected so that their sum is equal to the $R^2$ score of the full prediction $\\hat{y}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqBA1ZOE7lek"
      },
      "outputs": [],
      "source": [
        "split_scores_mask = {}\n",
        "\n",
        "Y_test_pred_split_lh = pipe_lh.predict(np.hstack((img_feats_test, lang_feats_test)), split=True)\n",
        "split_scores_mask['left'] = backend.to_numpy(r2_score_split(lh_fmri_test, Y_test_pred_split_lh))\n",
        "\n",
        "Y_test_pred_split_rh = pipe_rh.predict(np.hstack((img_feats_test, lang_feats_test)), split=True)\n",
        "split_scores_mask['right'] = backend.to_numpy(r2_score_split(rh_fmri_test, Y_test_pred_split_rh))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xe4Ksj39-_V_"
      },
      "outputs": [],
      "source": [
        "hemisphere = 'left' #@param [\"left\", \"right\"]\n",
        "\n",
        "view = plot_surf_map(stat_map=split_scores_mask[hemisphere][0],\n",
        "                     subj=subj, hemi=hemisphere, title='Vision model scores')\n",
        "view"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nCPpxWsYF9RZ"
      },
      "outputs": [],
      "source": [
        "hemisphere = 'left' #@param [\"left\", \"right\"]\n",
        "\n",
        "view = plot_surf_map(stat_map=split_scores_mask[hemisphere][1],\n",
        "                     subj=subj, hemi=hemisphere, title='Language model scores')\n",
        "view"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HH3u4qJBiIkx"
      },
      "source": [
        "What can you see by looking at these brain maps? Where does the vision model perform better, and where does the language model perform better instead?\n",
        "\n",
        "In this final part of the tutorial, we have seen another method, beyond variance partitioning, to disentangle the roles of different models in predicting fMRI data. As in cognitive science and AI we are often dealing with messy real-world datasets, where variables often correlate with each other, disentangling the contributions of different models is a problem we will constantly face!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}