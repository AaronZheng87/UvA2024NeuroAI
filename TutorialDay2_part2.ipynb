{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cognition-And-Vision-Amsterdam-CAVA/UvA2024NeuroAI/blob/main/TutorialDay2_part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial Day 2 - SPoSE\n",
        "\n",
        "by Giacomo Aldegheri\n",
        "\n",
        "\\\n",
        "\\\n",
        "In this tutorial, we will look at **SPoSE** (Sparse Positive Object Similarity Embeddings) a model proposed by Martin Hebart and colleagues in a [2020 paper](https://www.nature.com/articles/s41562-020-00951-3) and then in a [follow-up paper](https://elifesciences.org/articles/82580) to estimate the dimensions underlying human judgments of image similarities. What does that mean?\n",
        "\n",
        "They ran a large-scale online behavioral study, in which they simply showed, on each trial, 3 images, and asked participants to indicate which was the odd one out (the image most dissimilar to the others):\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1zONI4kRZmsP0jlO-tNgPYy46EhVsHXVL' width=400>\n",
        "\n",
        "They did this for 1854 object concepts (from the [THINGS database](https://things-initiative.org/)) and 1.46 million trials.\n",
        "\n",
        "Using this data, they tried to understand the dimensions underlying subjects' similarity judgments. They used the following procedure:\n",
        "\n",
        "\\\n",
        "<img src='https://drive.google.com/uc?id=1zPHHATd117y1YWGSs4FbRoD9FI5TWGjh' width=750>\n",
        "\n",
        "\\\n",
        "In essence, they pre-specified a number of embedding dimensions (e.g. 40) and created a simple linear model with a `1854 x 40` (n. concepts x n. dimensions) weight matrix. They fed each concept as a 1854-dimensional one-hot vector to the model, and transformed it into a 40-dimensional embedding.\n",
        "\n",
        "The embeddings were extracted for each of the three items in a triplet (**step 1** in the figure above), and their pairwise similarity was computed with a dot product (**step 2**). These dot products were turned into choice probabilities using the [softmax function](https://en.wikipedia.org/wiki/Softmax_function) (**step 3**), and the model's choice (pair with the highest probability) was compared to a human subject's choice (**step 4**).\n",
        "\n",
        "The idea is that among the three pairs, the highest probability (pairwise similarity) should be assigned to the pair that does *not* include the odd-one-out. For example, if from the triplet `glass, bottle, car` I judge the car to be the odd-one-out, the model should predict the pair `glass, bottle` to have a higher similarity than `glass, car` and `bottle, car`. The distance ([cross-entropy loss](https://en.wikipedia.org/wiki/Cross-entropy)) between the model's choice and the correct choice was then [backpropagated](https://en.wikipedia.org/wiki/Backpropagation) to the weights, making them increasingly informative about subjects' judgments.\n",
        "\n",
        "They also put two additional constraints on the weights:\n",
        "\n",
        "- They had to be **positive**: the intuition is that each weight should reflect the presence, or absence, of a given feature. E.g. an animal can be more or less furry, but it can't be negative furry.\n",
        "\n",
        "- They had to be **sparse**: (for any given input, most features' activations should be 0) when a lot of features are present for each object, they are usually not very interpretable. Only a few features should be active for any given object.\n",
        "\n",
        "Both of these constraints were informed using special loss functions, as we will see below.\n",
        "\n",
        "The resulting features turn out to reflect interpretable concepts, that reflect the dimensions along which people's internal representations of objects are organized.\n",
        "\n",
        "\\\n",
        "With all that in mind, time to dive into the code! This tutorial closely follows the [official implementation](https://github.com/ViCCo-Group/SPoSE).\n",
        "\n"
      ],
      "metadata": {
        "id": "x34vkLhKHD_v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries"
      ],
      "metadata": {
        "id": "6cQrYT1ZfO4R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHK3ehScGhpJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "from tqdm.notebook import tqdm\n",
        "from typing import Tuple\n",
        "import random\n",
        "\n",
        "rand_seed = 123\n",
        "random.seed(rand_seed)\n",
        "np.random.seed(rand_seed)\n",
        "torch.manual_seed(rand_seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set device\n",
        "\n",
        "If possible, use the GPU for much faster computation!"
      ],
      "metadata": {
        "id": "lVU7-o4IfSfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' # @param ['cuda', 'cpu']\n",
        "if device == 'cuda':\n",
        "  assert torch.cuda.is_available(), 'GPU not available! Please select a GPU runtime or use CPU.'\n",
        "device = torch.device(device)"
      ],
      "metadata": {
        "id": "i5gzeeAfABVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive\n",
        "\n",
        "You should have already added a shortcut to the data folder in your Google Drive during yesterday's tutorial. If not, this is how you do it:\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=15TNjV__sWCcnBRlxbXNbJfpidx-C6nrk' width=500>\n",
        "\n",
        "Go to the [folder](https://drive.google.com/drive/folders/1AjDOejWLjfXGkr-hK07SZJ_4ni1nypjw?usp=sharing), right click on its name, and select `Organize -> Add shortcut`. It will add a shortcut to your own Google Drive without the need to copy any data or occupy any storage."
      ],
      "metadata": {
        "id": "UVp38XEPfY8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "data_dir = '/content/drive/MyDrive/UvA_encodingtutorial/SPoSE/triplet_dataset/'"
      ],
      "metadata": {
        "id": "ufbBY0c1Hgud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Specify directory to save trained model parameters\n",
        "\n",
        "After we have trained the model (as well as during training, to see how the weights evolve) we want to save the model's weights.\n",
        "\n",
        "This should be on your own Google Drive, not in the shared folder, where you don't have writing permissions."
      ],
      "metadata": {
        "id": "GcRxBpA8dR5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_dir = 'SPoSE_weights' #@param {type: 'string'}\n",
        "log_dir = f'/content/drive/MyDrive/{log_dir}/'\n",
        "if not os.path.isdir(log_dir):\n",
        "  os.makedirs(log_dir)"
      ],
      "metadata": {
        "id": "ZOyz2YDudG2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Utility functions\n",
        "\n",
        "def load_triplets(partition:str, data_dir=data_dir):\n",
        "  if partition == 'train':\n",
        "    fname = 'trainset.txt'\n",
        "    n_items = 500000\n",
        "  elif partition == 'val':\n",
        "    fname = 'validationset.txt'\n",
        "    n_items = 20000\n",
        "  elif partition == 'test':\n",
        "    fname = 'testset1.txt'\n",
        "    n_items = 10000\n",
        "\n",
        "  triplets = np.loadtxt(os.path.join(data_dir, fname))\n",
        "\n",
        "  return torch.from_numpy(triplets[:n_items]).type(torch.LongTensor)\n",
        "\n",
        "def accuracy_(probas:np.ndarray) -> float:\n",
        "    choices = np.where(probas.mean(axis=1) == probas.max(axis=1), -1, np.argmax(probas, axis=1))\n",
        "    acc = np.where(choices == 0, 1, 0).mean()\n",
        "    return acc\n",
        "\n",
        "def choice_accuracy(anchor:torch.Tensor, positive:torch.Tensor, negative:torch.Tensor) -> float:\n",
        "    similarities  = compute_similarities(anchor, positive, negative)\n",
        "    probas = F.softmax(torch.stack(similarities, dim=-1), dim=1).detach().cpu().numpy()\n",
        "    return accuracy_(probas)\n",
        "\n",
        "def filter_nonneg(W, threshold=0.1):\n",
        "  W = W*(W>threshold)\n",
        "  return W"
      ],
      "metadata": {
        "id": "Dc9acn-vX8TM",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "Here, to speed up computations, we will only use a subset of the available. Specifically, this dataset contains 4.12M triplets, but we will only use 500K for training. It's less than 1/8 of the data, but we will see that it works quite well! For validation, we will use 20K."
      ],
      "metadata": {
        "id": "v2pyJ34pi53K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_triplets = load_triplets('train')\n",
        "val_triplets = load_triplets('val')\n",
        "\n",
        "print('Number of training samples:', len(train_triplets))\n",
        "print('Number of validation samples:', len(val_triplets))"
      ],
      "metadata": {
        "id": "BWqbH_-QZH6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data contains 1854 concepts, as we can verify:"
      ],
      "metadata": {
        "id": "ufguSJ6h3jf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_items = len(torch.unique(train_triplets))\n",
        "print('N. unique concepts:', n_items)"
      ],
      "metadata": {
        "id": "L_6FQOUOI255"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a dataset to feed the triplets to our model. First, the unique concept IDs need to be coded as one-hot vectors (1854-dimensional, with 1 for the current concept's entry and 0 elsewhere)."
      ],
      "metadata": {
        "id": "A5uvGlc75YvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_as_onehot(I:torch.Tensor, triplets:torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"encode item triplets as one-hot-vectors\"\"\"\n",
        "    return I[triplets.flatten(), :]\n",
        "\n",
        "\n",
        "class TripletDataset(Dataset):\n",
        "\n",
        "    def __init__(self, n_items:int, dataset:torch.Tensor):\n",
        "        self.I = torch.eye(n_items)\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx:int) -> torch.Tensor:\n",
        "        sample = encode_as_onehot(self.I, self.dataset[idx])\n",
        "        return sample"
      ],
      "metadata": {
        "id": "fRbcaHpcSpu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model\n",
        "\n",
        "Finally, time to code the actual model! As you can see, it's really simple. Just a single linear layer (only weights, no biases), with the number of concepts as input size and the number of embedding dimensions as output size.\n",
        "\n",
        "**EXERCISE:** fill in the code for the single linear layer in the model."
      ],
      "metadata": {
        "id": "GkY3NlJm9ntq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SPoSE(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "                self,\n",
        "                in_size:int,\n",
        "                out_size:int,\n",
        "                init_weights:bool=True,\n",
        "                ):\n",
        "        super(SPoSE, self).__init__()\n",
        "        self.in_size = in_size\n",
        "        self.out_size = out_size\n",
        "        # YOUR CODE HERE\n",
        "        #self.fc = ...\n",
        "\n",
        "        if init_weights:\n",
        "            self._initialize_weights()\n",
        "\n",
        "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
        "        return self.fc(x)\n",
        "\n",
        "    def _initialize_weights(self) -> None:\n",
        "        mean, std = .1, .01\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                m.weight.data.normal_(mean, std)"
      ],
      "metadata": {
        "id": "dl082nKC9bjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss functions\n",
        "\n",
        "And now one of the most important ingredients: the loss functions. First we code the two regularizers: the L1 regularization to enforce sparsity, and the positivity penalty to enforce positive weights.\n",
        "\n",
        "- For **sparsity**, we use the [L1 loss](https://towardsdatascience.com/intuitions-on-l1-and-l2-regularisation-235f2db4c261): $\\sum_{i=1}^n|\\textbf{W}_i|$ for each weight $\\textbf{W}_i$.\n",
        "\n",
        "- For **positivity**, we use the sum of all weights less than 0 as a loss term: $\\sum_{i=1}^{n}\\operatorname{ReLU}(-\\textbf{W}_i)$ for each weight $\\textbf{W}_i$.\n",
        "\n",
        "\\\n",
        "**EXERCISE:** fill in the code for the positivity loss."
      ],
      "metadata": {
        "id": "TTNNmTvI-7vD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def l1_regularization(model, device=device) -> torch.Tensor:\n",
        "    l1_reg = torch.tensor(0., requires_grad=True)\n",
        "    for n, p in model.named_parameters():\n",
        "        if re.search(r'weight', n):\n",
        "            l1_reg = l1_reg + torch.norm(p, 1)\n",
        "    return l1_reg.to(device)\n",
        "\n",
        "def pos_penalty(model) -> torch.Tensor:\n",
        "  W = model.fc.weight\n",
        "  # YOUR CODE HERE:\n",
        "  #return ... # positivity constraint to enforce non-negative values in embedding matrix"
      ],
      "metadata": {
        "id": "uqheGiZf-9Zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we code our main loss function, `trinomial_loss`.\n",
        "\n",
        "It's based on the [cross-entropy](https://en.wikipedia.org/wiki/Cross-entropy) loss:\n",
        "\n",
        "$$\n",
        "H(p, q) = -\\sum_{x \\in \\mathcal{X}}p(x)\\log{q(x)}\n",
        "$$\n",
        "\n",
        "Where $p$ and $q$ are two probability distributions, corresponding to the ground-truth distribution and the model's outputs.\n",
        "\n",
        "Basically, it makes sure the similarities between the triplets correspond to the choices in the odd-one-out task.\n",
        "\n",
        "First, we need to compute the similarities between the embeddings (using the dot product).\n",
        "\n",
        "Then, since in the dataset the chosen pair (that doesn't include the odd-one-out) is always the first in the triplet, we just need to ensure that the model's estimated choice probability/similarity for the first pair is always the highest.\n",
        "\n",
        "**EXERCISE:** implement the dot product computation for the three pairs: `pos_sim` (anchor, positive), `neg_sim` (anchor, negative) and `neg_sim_2` (positive, negative)"
      ],
      "metadata": {
        "id": "TF9lmOgdCyGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_similarities(anchor:torch.Tensor, positive:torch.Tensor, negative:torch.Tensor) -> Tuple:\n",
        "    # YOUR CODE HERE\n",
        "    # pos_sim = ...\n",
        "    # neg_sim = ...\n",
        "    # neg_sim_2 = ...\n",
        "\n",
        "    return pos_sim, neg_sim, neg_sim_2\n",
        "\n",
        "def weighted_softmax(sims: tuple, t:float) -> torch.Tensor:\n",
        "  return torch.exp(sims[0] / t) / torch.sum(torch.stack([torch.exp(sim / t) for sim in sims]), dim=0)\n",
        "\n",
        "def cross_entropy_loss(sims:tuple, t:float) -> torch.Tensor:\n",
        "    return torch.mean(-torch.log(weighted_softmax(sims, t)))\n",
        "\n",
        "def trinomial_loss(anchor:torch.Tensor, positive:torch.Tensor, negative:torch.Tensor, t:float) -> torch.Tensor:\n",
        "  sims = compute_similarities(anchor, positive, negative)\n",
        "  return cross_entropy_loss(sims, t)"
      ],
      "metadata": {
        "id": "x254k-rZCxN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters\n",
        "\n",
        "Now we're all set! We just need to set a few hyperparameters:\n",
        "\n",
        "- `lmbda`: this is the weight of the l1-regularization (we can't use the name `lambda` as it's a Python keyword 😁)\n",
        "- `temperature`: the temperature for the softmax function.\n",
        "- `lr`: the learning rate.\n",
        "- `batch_size`: the batch size for training.\n",
        "- `embed_dim`: the embedding's dimensionality.\n",
        "- `n_epochs`: number of training epochs."
      ],
      "metadata": {
        "id": "qEDOVpe9IRKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lmbda = 0.02\n",
        "temperature = 1.\n",
        "lr = 0.001\n",
        "batch_size = 100\n",
        "embed_dim = 40\n",
        "n_epochs = 16"
      ],
      "metadata": {
        "id": "IPfkf1XKGdvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define our model, datasets, dataloaders and the optimizer."
      ],
      "metadata": {
        "id": "O8hfa7dlL6eQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SPoSE(in_size=n_items, out_size=embed_dim, init_weights=True)\n",
        "model = model.to(device)\n",
        "\n",
        "# Training/validation datasets and dataloaders:\n",
        "trainset = TripletDataset(n_items, train_triplets)\n",
        "validationset = TripletDataset(n_items, val_triplets)\n",
        "trainloader = DataLoader(dataset=trainset, batch_size=batch_size, shuffle=True)\n",
        "valloader = DataLoader(dataset=validationset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Optimizer (Adam with default settings)\n",
        "optim = Adam(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "JqGppyXEL5jB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training loop\n",
        "\n",
        "Ok, now it's time to train the model! With a GPU backend, it should take around 10 minutes."
      ],
      "metadata": {
        "id": "2zeXleu0WeJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crossentropies = []\n",
        "complexity_losses = []\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "\n",
        "for epoch in tqdm(range(n_epochs)):\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  # To keep track of the losses\n",
        "  batch_crossentropies = torch.zeros(len(trainloader))\n",
        "  batch_complosses = torch.zeros(len(trainloader))\n",
        "  batch_losses_train = torch.zeros(len(trainloader))\n",
        "  batch_accs_train = torch.zeros(len(trainloader))\n",
        "\n",
        "  for i, batch in enumerate(trainloader):\n",
        "    optim.zero_grad()\n",
        "    batch = batch.to(device)\n",
        "    logits = model(batch)\n",
        "\n",
        "    # separate the three embeddings:\n",
        "    anchor, positive, negative = torch.unbind(logits, dim=1)\n",
        "\n",
        "    c_entropy = trinomial_loss(anchor, positive, negative, temperature)\n",
        "    l1_pen = (lmbda/n_items) * l1_regularization(model, device=device)\n",
        "    pos_pen = pos_penalty(model)\n",
        "\n",
        "    # Sum everything into one big loss:\n",
        "    loss = c_entropy + 0.01 * pos_pen + l1_pen\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "    batch_losses_train[i] += loss.item()\n",
        "    batch_crossentropies[i] += c_entropy.item()\n",
        "    batch_complosses[i] += l1_pen.item()\n",
        "    batch_accs_train[i] += choice_accuracy(anchor, positive, negative)\n",
        "\n",
        "  avg_crossentropy = torch.mean(batch_crossentropies).item()\n",
        "  avg_comploss = torch.mean(batch_complosses).item()\n",
        "  avg_train_loss = torch.mean(batch_losses_train).item()\n",
        "  avg_train_acc = torch.mean(batch_accs_train).item()\n",
        "\n",
        "\n",
        "  ####################################\n",
        "  # Validation\n",
        "  ####################################\n",
        "\n",
        "  val_accs = torch.zeros(len(valloader))\n",
        "  val_losses = torch.zeros(len(valloader))\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for i, batch in enumerate(valloader):\n",
        "\n",
        "      batch = batch.to(device)\n",
        "      logits = model(batch)\n",
        "      anchor, positive, negative = torch.unbind(logits, dim=1)\n",
        "\n",
        "      val_loss = trinomial_loss(anchor, positive, negative, temperature)\n",
        "      val_acc = choice_accuracy(anchor, positive, negative)\n",
        "\n",
        "      val_losses[i] += val_loss.item()\n",
        "      val_accs[i] += val_acc.item()\n",
        "\n",
        "  avg_val_loss = torch.mean(val_losses).item()\n",
        "  avg_val_acc = torch.mean(val_accs).item()\n",
        "\n",
        "\n",
        "  print('\\n==========================================================')\n",
        "  print(f'Epoch: {epoch+1}, Train acc: {avg_train_acc:.5f}, Train loss: {avg_train_loss:.5f}, Val acc: {avg_val_acc:.5f}, Val loss: {avg_val_loss:.5f}')\n",
        "  print('==========================================================\\n')\n",
        "\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    # Save the weights every other epoch:\n",
        "    W = model.fc.weight.detach().cpu().numpy().T\n",
        "    np.savetxt(os.path.join(log_dir, f'weights_epoch{epoch+1:04d}.txt'), W)\n",
        "\n",
        "# Save final model\n",
        "W = model.fc.weight.detach().cpu().numpy().T\n",
        "np.savetxt(os.path.join(log_dir, 'weights_final.txt'), W)"
      ],
      "metadata": {
        "id": "hn_9SEg4Mi9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If all went well, you should have reached ~64% accuracy on the validation set! That's quite good, given that we are using less than 1/8 of the original dataset for training! Now, let's see what the embedding dimensions we have learned look like. Are they interpretable?"
      ],
      "metadata": {
        "id": "bjlOqywHifwb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspect the learned dimensions\n",
        "\n",
        "Let's code some visualization utilities."
      ],
      "metadata": {
        "id": "0XKNEriiiNTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Visualization utilities\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import gridspec\n",
        "import matplotlib.image as mpimg\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "import pandas as pd\n",
        "from glob import glob\n",
        "from scipy.io import loadmat\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "def plot_weights_across_time(log_dir=log_dir, n_epochs=n_epochs, n_rows=100):\n",
        "\n",
        "    all_epochs = np.arange(2, n_epochs+2, 2)\n",
        "\n",
        "    fig = plt.figure(figsize=(15, 5))\n",
        "    gs = gridspec.GridSpec(1, len(all_epochs)+1, width_ratios=[1]*len(all_epochs) + [0.05])\n",
        "\n",
        "    allweights = []\n",
        "    for epoch in all_epochs:\n",
        "        thisfile = os.path.join(log_dir, f'weights_epoch{epoch:04d}.txt')\n",
        "        allweights.append(filter_nonneg(np.loadtxt(thisfile)[:n_rows]))\n",
        "    allweights = np.dstack(allweights)\n",
        "\n",
        "    vmin = allweights.min()\n",
        "    vmax = allweights.max()\n",
        "\n",
        "    axes = []\n",
        "    for i, epoch in enumerate(all_epochs):\n",
        "        ax = fig.add_subplot(gs[0, i])\n",
        "        cax = ax.matshow(allweights[:,:,i], cmap='viridis', vmin=vmin, vmax=vmax)\n",
        "        ax.set_title(f'Epoch {epoch}')\n",
        "\n",
        "        # Remove ticks and labels\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        ax.tick_params(labelbottom=False, labelleft=False)\n",
        "\n",
        "        axes.append(ax)\n",
        "\n",
        "    # Add a colorbar to the right of the last subplot\n",
        "    cbar_ax = fig.add_subplot(gs[0, -1])\n",
        "    fig.colorbar(cax, cax=cbar_ax)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def get_top_k(W, dim, k):\n",
        "\n",
        "  sorted_indices = np.argsort(W[:, dim])\n",
        "  topk_indices = list(sorted_indices[-k:][::-1])\n",
        "\n",
        "  return topk_indices\n",
        "\n",
        "def show_top_concepts(W, dim, k, concept_list, img_dir):\n",
        "\n",
        "  fig, axes = plt.subplots(1, k, figsize=(k*5,5))\n",
        "  axes = axes.flatten()\n",
        "\n",
        "  topk_indices = get_top_k(W, dim, k)\n",
        "  for ax, i in zip(axes, topk_indices):\n",
        "    this_concept = concept_list[i]\n",
        "    concept_dir = glob(os.path.join(img_dir, this_concept+'*'))\n",
        "    this_img = random.choice(glob(os.path.join(img_dir, this_concept, '*.jpg')))\n",
        "    this_img = mpimg.imread(this_img)\n",
        "    ax.imshow(this_img)\n",
        "    ax.set_title(this_concept.replace('_', ' '), fontsize=22)\n",
        "    ax.axis('off')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "def check_rdm_size(rdmA, rdmB):\n",
        "\n",
        "  assert rdmA.shape == rdmB.shape, 'RDMs must have the same size!'\n",
        "  assert rdmA.shape[0] == rdmA.shape[1], 'RDMs must be square!'\n",
        "\n",
        "def plot_rdms(realrdm, modelrdm):\n",
        "\n",
        "  fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "  # Plot human RDM\n",
        "  im1 = axs[0].imshow(realrdm, cmap='viridis')\n",
        "  divider1 = make_axes_locatable(axs[0])\n",
        "  cax1 = divider1.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
        "  fig.colorbar(im1, cax=cax1)\n",
        "  axs[0].set_title('Human RDM', fontsize=22, pad=15)\n",
        "\n",
        "  # Plot model RDM\n",
        "  im2 = axs[1].imshow(modelrdm, cmap='viridis')\n",
        "  divider2 = make_axes_locatable(axs[1])\n",
        "  cax2 = divider2.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
        "  fig.colorbar(im2, cax=cax2)\n",
        "  axs[1].set_title('Model RDM', fontsize=22, pad=15)\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "def compute_rdm_correlation(rdmA, rdmB):\n",
        "\n",
        "  check_rdm_size(rdmA, rdmB)\n",
        "\n",
        "  loweridx = np.tril_indices(rdmA.shape[0], k=-1)\n",
        "\n",
        "  rdmA = rdmA[loweridx]\n",
        "  rdmB = rdmB[loweridx]\n",
        "\n",
        "  return pearsonr(rdmA, rdmB)[0]\n"
      ],
      "metadata": {
        "id": "VgEyWM-N2sRH",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot weights across time\n",
        "\n",
        "We plot, across training epochs, what the weights of our model look like. Can you notice some structure emerging?"
      ],
      "metadata": {
        "id": "2IxAwEHglX0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_weights_across_time(n_epochs=n_epochs)"
      ],
      "metadata": {
        "id": "B1IqHolvh6Lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Show dimensions\n",
        "\n",
        "For any of our 40 model dimensions, we want to see examples of concepts that maximally activate them. Do the images have anything in common? You can try to make sense of what the different dimensions correspond to, and perhaps name them."
      ],
      "metadata": {
        "id": "hspmTrdll476"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment this if you need to load the model's final weights again\n",
        "# (e.g. the runtime got disconnected)\n",
        "#W = np.loadtxt(os.path.join(log_dir, 'weights_final.txt'))"
      ],
      "metadata": {
        "id": "gm4mhQEGkVpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get list of concept names and image directory path\n",
        "\n",
        "things_concepts = pd.read_csv(os.path.join(data_dir, 'things_concepts.tsv'), sep='\\t')\n",
        "concept_list = list(things_concepts['uniqueID'].values)\n",
        "img_dir = os.path.join(data_dir, 'images')"
      ],
      "metadata": {
        "id": "Gt7xfWo7mkuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pick a few dimensions and plot some example concepts/images from each:"
      ],
      "metadata": {
        "id": "oqRbftd3l3Mo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dim = 0 # @param {type:\"slider\", min:0, max:39, step:1}\n",
        "show_top_concepts(W, dim=dim, k=5, concept_list=concept_list, img_dir=img_dir)"
      ],
      "metadata": {
        "id": "Cy2gqNjCnCND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dim = 29 # @param {type:\"slider\", min:0, max:39, step:1}\n",
        "show_top_concepts(W, dim=dim, k=5, concept_list=concept_list, img_dir=img_dir)"
      ],
      "metadata": {
        "id": "PUE8gc_Ulnkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dim = 18 # @param {type:\"slider\", min:0, max:39, step:1}\n",
        "show_top_concepts(W, dim=dim, k=5, concept_list=concept_list, img_dir=img_dir)"
      ],
      "metadata": {
        "id": "m0Sj-EqFlsdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare model and human RDM\n",
        "\n",
        "As a final test of the model we have learned, we check how well it can predict an RDM obtained from human behavioral judgments. After all, this is an RSA tutorial...\n",
        "\n",
        "Why is this a non-trivial task? Because in our training data, only a subset of possible concept pairs was \"seen\" by the model. The model, then, needs to reconstruct the full RDM from a sample. Here, we test it on an RDM of 48x48 concepts.\n",
        "\n",
        "**NOTE:** we are actually turning the Representational **DIS**similarity Matrix into a Representational **Similarity** Matrix, to directly compare it with the dot products (similarities) generated by our model. We still call it an RDM just to make it more confusing.\n",
        "\n",
        "**EXERCISE:** from the onehot encodings of the 48 concepts in the RDM, and the weight matrix, compute the embeddings. Then, from the embeddings, compute the matrix of pairwise dot products."
      ],
      "metadata": {
        "id": "RrXp9HVQCPgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the RDM and turn it into an \"RSM\":\n",
        "rdm48 = loadmat(os.path.join(data_dir, 'RDM48_triplet.mat'))['RDM48_triplet']\n",
        "rdm48 = 1. - rdm48\n",
        "\n",
        "# Load the list of 48 concepts used for this RDM, so we can feed them to the model:\n",
        "words48 = [w[0][0] for w in loadmat(os.path.join(data_dir, 'words48.mat'))['words48']]\n",
        "ids48 = []\n",
        "\n",
        "for w in words48:\n",
        "  w = w.replace(' ', '_')\n",
        "  if w not in concept_list:\n",
        "    pattern = re.compile(f\"^{re.escape(w)}\\d+$\")\n",
        "    theseconcepts = [c for c in concept_list if pattern.match(c)]\n",
        "    w = random.choice(theseconcepts)\n",
        "  ids48.append(concept_list.index(w))\n",
        "\n",
        "# Encode them as one-hot vectors:\n",
        "onehot48 = np.eye(n_items)[np.array(ids48)]\n",
        "\n",
        "# Get the embeddings by feeding the one-hot vectors\n",
        "# to the network:\n",
        "# YOUR CODE HERE\n",
        "# embeddings48 = ...\n",
        "\n",
        "# From the 48 x 40 embeddings matrix, get the 40 x 40 matrix\n",
        "# of rowwise dot products:\n",
        "# YOUR CODE HERE\n",
        "# rdm48_model = ..."
      ],
      "metadata": {
        "id": "caUKWvzjm5Wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the two RDMs side by side\n",
        "plot_rdms(rdm48, rdm48_model)"
      ],
      "metadata": {
        "id": "JZyKaqCp8-6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "They look quite similar! This is promising... let's check how correlated they are."
      ],
      "metadata": {
        "id": "0cbVRtx5p2V1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compute_rdm_correlation(rdm48, rdm48_model)"
      ],
      "metadata": {
        "id": "d8eh-Yxm2ija"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In conclusion, we can say that:\n",
        "\n",
        "- SPoSE can learn to accurately predict human choices in a triplet odd-one-out task.\n",
        "\n",
        "- It does so by generating interpretable dimensions underlying human judgments.\n",
        "\n",
        "- It is able to generalize and predict full pairwise similarity matrices from a separate experiment.\n",
        "\n",
        "Really not bad for only having 1854 x 40 linear weights!\n",
        "\n"
      ],
      "metadata": {
        "id": "tB03nwuPqH5-"
      }
    }
  ]
}